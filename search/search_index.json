{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Openshift Operators","text":""},{"location":"#about-this-repository","title":"About this repository","text":"<p>This repo is the canonical source for Kubernetes Operators that appear on OpenShift Container Platform and OKD.</p> <p>NOTE The index catalogs:</p> <ul> <li><code>registry.redhat.io/redhat/certified-operator-index:v&lt;OCP Version&gt;</code></li> <li><code>registry.redhat.io/redhat/redhat-marketplace-index:v&lt;OCP Version&gt;</code></li> <li><code>registry.redhat.io/redhat/community-operator-index:v&lt;OCP Version&gt;</code></li> </ul> <p>are built from this repository and it is consumed by Openshift and OKD to create their sources and built their catalog. To know more about how Openshift catalog are built see the documentation.</p> <p>See our documentation to find out more about Community, Certified and Marketplace operators and contribution.</p>"},{"location":"#add-your-operator","title":"Add your Operator","text":"<p>We would love to see your Operator added to this collection. We currently use automated vetting via continuous integration plus manual review to curate a list of high-quality, well-documented Operators. If you are new to Kubernetes Operators start here.</p> <p>If you have an existing Operator read our contribution guidelines on how to open a PR. Then the community operator pipeline will be triggered to test your Operator and merge a Pull Request.</p>"},{"location":"#remove-your-operator","title":"Remove Your Operator","text":"<p>Before You Begin Ensure your operator follows the FBC (File-Based Catalog) workflow. Setting fbc.enabled: true in the ci.yaml file is not enough. The operator must be fully onboarded to FBC. For non-FBC (bundle-based) operators, refer to the FBC onboarding guide before continuing.</p> <p>Depending on your use case, you may: - Remove the operator entirely from all catalogs. - Remove it from specific catalog version(s). - Remove a single operator version from the catalog.</p>"},{"location":"#remove-the-entire-operator-from-the-catalog","title":"Remove the Entire Operator from the Catalog","text":"<p>To remove the operator completely from the catalog:</p> <ul> <li>Delete the your operator directory from the <code>operators/</code> folder.</li> <li>Remove all catalog files related to your operator from the <code>catalogs/</code> directory.</li> <li>Open a single pull request that includes these changes. Follow our contribution guidelines on how to open a PR.</li> </ul> <p>For reference, here\u2019s an example PR demonstrating these steps.</p>"},{"location":"#remove-the-operator-from-specific-catalog-versions","title":"Remove the Operator from Specific Catalog Version(s)","text":"<p>To remove your operator from selected catalog versions:</p> <ul> <li>In <code>operators/&lt;operator-name&gt;/ci.yaml</code>, locate the <code>fbc.catalog_mapping</code> section and remove the targeted catalog version(s) from the <code>catalog_names</code> list.</li> </ul> <p>Example:</p> <pre><code>fbc:\n  enabled: true\n  catalog_mapping:\n    - template_name: basic.yaml\n      catalog_names: [\"v4.14\", \"v4.15\", \"v4.16\"]\n      type: olm.template.basic\n</code></pre> <p>To remove <code>v4.15</code>, update <code>catalog_names</code> to:</p> <pre><code>catalog_names: [\"v4.14\", \"v4.16\"]\n</code></pre> <ul> <li>From the <code>operators/&lt;operator-name&gt;/catalog-templates/</code> directory, delete any template YAML files that were associated with the removed catalog version(s), if applicable.</li> <li>Run <code>make catalog</code> to modify the catalog content. This will automatically remove all affected catalog files under <code>catalogs/</code>.</li> <li>Submit a single pull request with all these changes. Follow our PR guidelines.</li> </ul>"},{"location":"#remove-a-single-operator-bundle-version","title":"Remove a Single Operator Bundle Version","text":"<p>To remove a specific operator bundle version without affecting other versions:</p> <ul> <li>Remove the targeted operator version from the list of catalog-templates files located at <code>operators/&lt;your_operator&gt;/catalog-templates/</code>.</li> <li>Run <code>make catalog</code> to modify the catalog content. This will automatically update all affected catalog files under <code>catalogs/</code> by removing the specified version details.</li> <li>Remove the entire targeted operator bundle version subdirectory located at <code>operators/&lt;your_operator_name&gt;/</code>.</li> <li>Submit a single pull request that includes this change.</li> </ul> <p>For reference, see this example pull request.</p>"},{"location":"#contributing-guide","title":"Contributing Guide","text":"<ul> <li>Prerequisites</li> <li>Where to place operator</li> <li>Creating pull request (PR)</li> <li>Operator Publishing / Review settings</li> <li>OKD/OpenShift Catalogs criteria and options</li> </ul>"},{"location":"#test-and-release-process-for-the-operator","title":"Test and release process for the Operator","text":"<p>Refer to the operator pipeline documentation .</p>"},{"location":"#important-notice","title":"IMPORTANT NOTICE","text":"<p>Some APIs versions are deprecated and are OR will no longer be served on the Kubernetes version <code>1.22/1.25/1.26</code> and consequently on vendors like Openshift <code>4.9/4.12/4.13</code>.</p> <p>What does it mean for you?</p> <p>Operator bundle versions using the removed APIs can not work successfully from the respective releases. Therefore, it is recommended to check if your solutions are failing in these scenarios to stop using these versions OR by setting the <code>\"olm.properties\": '[{\"type\": \"olm.maxOpenShiftVersion\", \"value\": \"&lt;OCP version&gt;\"}]'</code> to block cluster admins upgrades when they have Operator versions installed that can not work well in OCP versions higher than the value informed. Also, by defining a valid OCP range via the annotation <code>com.redhat.openshift.versions</code> into the <code>metadata/annotations.yaml</code> for our solution does not end up shipped on OCP/OKD versions where it cannot be installed.</p> <p>WARNING: <code>olm.maxOpenShiftVersion</code> should ONLY be used if you are 100% sure that your Operator bundle version cannot work in upper releases. Otherwise, you might provide a bad user experience. Be aware that cluster admins will be unable to upgrade their clusters with your solution installed. Then, suppose you do not provide any upper version and a valid upgrade path for those who have your Operator installed be able to upgrade it and consequently be allowed to upgrade their cluster version (i.e from OCP 4.10 to 4.11). In that case, cluster admins might choose to uninstall your Operator and no longer use it so that they can move forward and upgrade their cluster version without it.</p> <p>Please, make sure you check the following announcements: - How to deal with removal of v1beta1 CRD removals in Kubernetes 1.22 / OpenShift 4.9 - Kubernetes API removals on 1.25/1.26 and Openshift 4.12/4.13 might impact your Operator. How to deal with it?</p>"},{"location":"#reporting-bugs","title":"Reporting Bugs","text":"<p>Use the issue tracker in this repository to report bugs.</p>"},{"location":"bootstrap-signing/","title":"Index bootstrap signing","text":"<p>Whenever there is a new release of the OpenShift we have to release a new version of operator catalog. This process involves several steps that involves several people and teams. To reduce amount of manual work we automated the process. One of the steps in the automation is to sign the catalog image with the Red Hat GPG key.</p> <p>This document describes how to sign the catalog image with the Red Hat GPG key and how to use signing automation.</p>"},{"location":"bootstrap-signing/#prerequisites","title":"Prerequisites","text":"<ul> <li>A new version of the index image is available at registry.redhat.io</li> <li>registry.redhat.io/redhat/community-operator-index</li> <li>registry.redhat.io/redhat/certified-operator-index</li> <li>An image contains a verion tag - <code>v4.18</code> for example</li> <li>Access to the OpenShift cluster with Tekton trigger permissions</li> <li><code>tkn</code> CLI installed</li> <li>A version that needs to be signed can't be already GA and stored in Pyxis indices</li> <li>https://catalog.redhat.com/api/containers/v1/operators/indices</li> <li>If the version is already GA, a workflow needs to be consulted with the Collective team</li> </ul>"},{"location":"bootstrap-signing/#sign-the-image","title":"Sign the image","text":"<p>The signing process is automated using Tekton pipeline. To trigger a pipeline a <code>tkn</code> cli command needs to be triggered.</p> <p>First, login to the OpenShift cluster:</p> <pre><code>oc login --server=https://api.pipelines-prod.ijdb.p1.openshiftapps.com:6443 --token=&lt;token&gt;\noc project index-bootstrap-prod # or index-bootstrap-stage\n</code></pre> <p>Then, create a volume claim template:</p> <pre><code>cat &lt;&lt;EOF &gt; workspace-template.yaml\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 50Mi\nEOF\n\n</code></pre> <p>Then, trigger the pipeline:</p> <pre><code>tkn pipeline start \\\n    index-img-bootstrap-signing-pipeline \\\n    --use-param-defaults \\\n    -p image_pullspec=registry.redhat.io/redhat/community-operator-index:v4.18 \\\n    -p env=prod \\\n    -p requester=&lt;person or team name&gt; \\\n    --showlog \\\n    --workspace name=pipeline,volumeClaimTemplateFile=./workspace-template.yaml\n</code></pre> <p>The pipeline will sign the image and store the signature to Container sigstore (Pyxis).</p>"},{"location":"bootstrap-signing/#stage-environment","title":"Stage environment","text":"<p>To sign the image for the stage environment (registry.stage.redhat.io), the <code>env</code> parameter needs to be set to <code>stage</code> and signing needs to be done on stage cluster.</p> <pre><code>oc login --server=https://api.pipelines-stage.0ce8.p1.openshiftapps.com:6443 --token=&lt;token&gt;\n</code></pre>"},{"location":"catalog_image_browser/","title":"Catalog image browser","text":"<p>Working with catalog images locally might be a tricky task since it requires a lot of manual work. To make it easier for maintainers and contributors, we have created a simple catalog image browser CLI tool that allows you to browse and search for images in the catalog locally in your terminal.</p>"},{"location":"catalog_image_browser/#installation","title":"Installation","text":"<p>To install the catalog browser, you will need a Python 3.10+ environment.</p> <p>Then, you can install the tool using pip:</p> <pre><code>$ pip install git+https://github.com/redhat-openshift-ecosystem/operator-pipelines.git\n</code></pre> <p>Once installed you can run the tool using the following command:</p> <pre><code>$ catalog-browser --help\n\nusage: catalog-browser [-h] [--image IMAGE] [--rendered RENDERED] {list,show} ...\n\nBrowse and query index image content.\n\npositional arguments:\n  {list,show}          Commands\n    list               List content in the index image.\n    show               Show details of specific content.\n\noptions:\n  -h, --help           show this help message and exit\n  --image IMAGE        Path to the index image.\n  --rendered RENDERED  Path to the rendered index image content.\n\n</code></pre>"},{"location":"catalog_image_browser/#usage","title":"Usage","text":"<p>The browser requires one of 2 argument inputs: <code>--image</code> or <code>--rendered</code>. Using <code>--image</code> argument the tool will pull the image and extract the content to a temporary directory. Based on catalog image size the extraction might take a while. You can also render a catalog in advance using <code>opm render</code> and then using <code>--rendered</code> argument to browse the content.</p> <pre><code>$ catalog-browser --image registry.redhat.io/redhat/community-operator-index:v4.16 list packages\n\n# or\n$ opm render -o yaml registry.redhat.io/redhat/community-operator-index:v4.16 &gt; /tmp/v.4.16.yaml\n\n$ catalog-browser --rendered /tmp/v.4.16.yaml list bundles\n</code></pre> <p>The browser supports 2 commands: <code>list</code> and <code>show</code>.</p>"},{"location":"catalog_image_browser/#list","title":"List","text":"<p>The <code>list</code> command will list all packages, bundles or channels</p> <pre><code>$ catalog-browser --image registry.redhat.io/redhat/community-operator-index:v4.16 list packages\n3scale-community-operator\nack-acm-controller\nack-acmpca-controller\nack-apigateway-controller\nack-apigatewayv2-controller\nack-applicationautoscaling-controller\nack-athena-controller\n...\n</code></pre> <pre><code>$ catalog-browser --rendered /tmp/v.4.16.yaml list bundles\n3scale-community-operator.v0.10.1\n3scale-community-operator.v0.8.2\n3scale-community-operator.v0.9.0\nack-acm-controller.v0.0.1\nack-acm-controller.v0.0.10\nack-acm-controller.v0.0.12\nack-acm-controller.v0.0.16\nack-acm-controller.v0.0.17\nack-acm-controller.v0.0.18\n...\n</code></pre>"},{"location":"catalog_image_browser/#show","title":"Show","text":"<p>The <code>show</code> command will show details of a specific package, bundle or channel in human readable format.</p> <p>To show details of a package:</p> <pre><code>$ catalog-browser --rendered /tmp/v.4.16.yaml show package tempo-operator\nPackage: tempo-operator\nChannels:\n - tempo-operator/alpha\nBundles:\n - tempo-operator.v0.1.0\n - tempo-operator.v0.10.0\n - tempo-operator.v0.11.0\n - tempo-operator.v0.11.1\n - tempo-operator.v0.12.0\n - tempo-operator.v0.13.0\n - tempo-operator.v0.14.0\n - tempo-operator.v0.14.1\n - tempo-operator.v0.14.2\n - tempo-operator.v0.2.0\n - tempo-operator.v0.3.0\n - tempo-operator.v0.4.0\n - tempo-operator.v0.5.0\n - tempo-operator.v0.6.0\n - tempo-operator.v0.7.0\n - tempo-operator.v0.8.0\n - tempo-operator.v0.9.0\n</code></pre> <p>To show details of a bundle:</p> <pre><code>$ catalog-browser --rendered /tmp/v.4.16.yaml show bundle snyk-operator.v1.90.2\nBundle: snyk-operator.v1.90.2\nPackage: snyk-operator\nImage: quay.io/openshift-community-operators/snyk-operator@sha256:daf143ff1e9fbcf9bbbb350f8aab8593a1a35f693b0118c06a6b84c89a474397\nChannels:\n - snyk-operator/stable\n</code></pre>"},{"location":"ci-cd/","title":"CI/CD","text":"<p>This project uses GitHub Actions and Ansible for CI (tests, linters) and CD (deployment).</p>"},{"location":"ci-cd/#secrets","title":"Secrets","text":"<p>Both deployment and integration tests need GitHub secrets to work properly. The following secrets should be kept in the repository:</p> Secret name Secret value Purpose VAULT_PASSWORD Password to the preprod Ansible Vault stored in the repository Deployment of preprod and integration test environments VAULT_PASSWORD_PROD Password to the prod Ansible Vault stored in the repository Deployment of the production environment REGISTRY_USERNAME Username for authentication to the container registry Building images REGISTRY_PASSWORD Password for authentication to the container registry Building images GITHUB_TOKEN GitHub authentication token Creation of GitHub tags and releases"},{"location":"ci-cd/#run-order","title":"Run order","text":"<ul> <li>Validation- run always.</li> <li>Build and Push Image/ ppc64le Image- run always. That is intended to help for the developers to test the image in case of changes.</li> <li>Integration Tests- runs only on the merge to <code>main</code> (or on manual trigger)</li> <li>Deployment- run only after integration tests pass on the <code>main</code> branch.</li> </ul>"},{"location":"ci-cd/#integration-tests","title":"Integration tests","text":""},{"location":"ci-cd/#when-do-they-run","title":"When do they run?","text":"<p>Integration tests are a stage of the CI/CD that runs only in two cases: - On merge to main and before deployment - On desire (manual action- by clicking \"run workflow\" via GitHub UI)</p>"},{"location":"ci-cd/#running-integration-tests","title":"Running integration tests","text":"<p>The orchestration of the integration tests is handled by Ansible. A couple dependencies must be installed to get started:</p> <ul> <li>Ansible</li> <li>Python packages: <code>openshift</code>, <code>pygithub</code></li> </ul> <p>To execute the integration tests in a custom environment:</p> <pre><code>ansible-pull \\\n  -U \"https://github.com/redhat-openshift-ecosystem/operator-pipelines.git\" \\\n  -i \"ansible/inventory/operator-pipeline-integration-tests\" \\\n  -e \"oc_namespace=$NAMESPACE\" \\\n  --vault-password-file $VAULT_PASSWORD_PATH \\\n  ansible/playbooks/operator-pipeline-integration-tests.yml\n</code></pre> <p>To manually run the integration tests from the local environment: - prerequisites:   - logged-in to OC cluster   - export NAMESPACE - new is created if not exist,     careful for duplicity (can override existing projects)   - SSH key need to be set in GitHub account for local user     (Ansible use SSH to clone/manipulate repositories)   - Python dependencies (mentioned above) need to be installed globally</p> <pre><code>ansible-playbook -v \\\n  -i \"ansible/inventory/operator-pipeline-integration-tests\" \\\n  -e \"oc_namespace=$NAMESPACE\" \\ \n  --vault-password-file $VAULT_PASSWORD_PATH \\\n  ansible/playbooks/operator-pipeline-integration-tests.yml\n</code></pre> <p>Tags can be used to run select portions of the playbook. For example, the test resources will be cleaned up at the end of every run. Skipping the <code>clean</code> tag will leave the resources behind for debugging.</p> <pre><code>ansible-pull \\\n  --skip-tags clean \\\n  -U \"https://github.com/redhat-openshift-ecosystem/operator-pipelines.git\" \\\n  -i \"ansible/inventory/operator-pipeline-integration-tests\" \\\n  -e \"oc_namespace=$NAMESPACE\" \\\n  --vault-password-file $VAULT_PASSWORD_PATH \\\n  ansible/playbooks/operator-pipeline-integration-tests.yml\n</code></pre> <p>It may be necessary to provide your own project and bundle to test certain aspects of the pipelines. This can be accomplished with the addition of a few extra vars (and proper configuration of the project).</p> <pre><code>ansible-pull \\\n  -U \"https://github.com/redhat-openshift-ecosystem/operator-pipelines.git\" \\\n  -i \"ansible/inventory/operator-pipeline-integration-tests\" \\\n  -e \"oc_namespace=$NAMESPACE\" \\\n  -e \"src_operator_git_branch=$SRC_BRANCH\" \\\n  -e \"src_operator_bundle_version=$SRC_VERSION\" \\\n  -e \"operator_package_name=$PACKAGE_NAME\" \\\n  -e \"operator_bundle_version=$NEW_VERSION\" \\\n  -e \"ci_pipeline_pyxis_api_key=$API_KEY\" \\\n  --vault-password-file $VAULT_PASSWORD_PATH \\\n  ansible/playbooks/operator-pipeline-integration-tests.yml\n</code></pre>"},{"location":"cluster-config/","title":"Cluster Configuration","text":"<p>All OpenShift clusters should share a common configuration for our pipelines. There are cluster-wide resources which require modification, such as the TektonConfig. But there is also a custom EventListener which reports PipelineRun events to Slack and a pipeline that uploads the metrics of other pipelines for monitoring purposes. This configuration must be applied manually for now.</p> <p>To apply these cluster-wide configurations, run the Ansible playbook. To only apply the cluster-wide resources, the following command will suffice.</p> <pre><code>ansible-playbook \\\n    -i inventory/clusters \\\n    -e \"clusters={INSERT ANSIBLE HOST LIST}\" \\\n    -e \"ocp_token={INSERT TOKEN}\" \\\n    -e \"k8s_validate_certs={yes|no}\" \\\n    --vault-password-file \"{INSERT FILE}\" \\\n    playbooks/config-ocp-cluster.yml\n</code></pre> <p>If you want to deploy the metrics pipeline, add <code>--tags metrics</code> to the above command. To deploy the Chat Webhook, add <code>--tags chat</code>. If you wish to deploy both, add <code>--tags metrics,chat</code>.</p>"},{"location":"developer-guide/","title":"Developer Guide","text":""},{"location":"developer-guide/#workflow","title":"Workflow","text":"<ol> <li>Run through the setup at least once.</li> <li>Make changes to the pipeline image,    if desired.</li> <li>Make changes to the Tekton pipelines and/or tasks,    if desired.</li> <li>Test all impacted pipelines.</li> <li>Override the pipeline image as necessary.</li> <li>Submit a pull request with your changes.</li> </ol>"},{"location":"developer-guide/#setup","title":"Setup","text":"<ol> <li>Git leaks detection</li> <li>Prepare a development environment</li> <li>Prepare a certification project</li> <li>Prepare an Operator bundle</li> <li>Prepare your <code>ci.yaml</code></li> <li>Create a bundle pull request (optional)</li> <li>Required for testing hosted or release pipelines</li> <li>Create an API key (optional)</li> <li>Required for testing submission with the CI pipeline</li> <li>Prepare the CI to run from your fork (optional)</li> <li>Required to run integration testing on forks of this repo.</li> </ol>"},{"location":"developer-guide/#git-leaks-detection","title":"Git leaks detection","text":"<p>Since the repository contains secret information in form of encrypted Ansible Vault there is high chance that developer may push a commit with decrypted secrets by mistake. To avoid this problem we recommend to use <code>Gitleaks</code> tool that prevent you from commit secret code into git history.</p> <p>The repository is already pre-configured but each developer has to make final config changes in his/her environment.</p> <p>Follow the documentation to configure Gitleaks on your computer.</p>"},{"location":"developer-guide/#prepare-a-development-environment","title":"Prepare a Development Environment","text":"<p>You may use any OpenShift 4.7+ cluster (including CodeReady Containers).</p> <p>The hosted and release pipelines require a considerable amount of dependencies which are tedious to configure manually. Luckily these steps have been automated and can be executed by anyone with access to the Ansible vault password.</p> <p>Before running this you should ensure you're logged into the correct OpenShift cluster using <code>oc</code>. If already logged into the OpenShift console, an <code>oc</code> login command can be obtained by clicking on your username in upper right corner, and selecting <code>copy login command</code>.</p> <pre><code>ansible-playbook -v \\\n  -i \"ansible/inventory/operator-pipeline-$ENV\" \\\n  -e \"oc_namespace=$NAMESPACE\" \\\n  -e \"ocp_token=`oc whoami -t`\" \\\n  --vault-password-file $VAULT_PASSWORD_PATH \\\n  ansible/playbooks/deploy.yml\n</code></pre> <p>:warning: Conflicts may occur if the project already contains some resources. They may need to be removed first.</p> <p>Cleanup can be performed by specifying the <code>absent</code> state for some of the resources.</p> <pre><code>ansible-playbook -v \\\n  -i \"ansible/inventory/operator-pipeline-$ENV\" \\\n  -e \"oc_namespace=$NAMESPACE\" \\\n  -e \"ocp_token=`oc whoami -t`\" \\\n  -e \"namespace_state=absent\" \\\n  -e \"github_webhook_state=absent\" \\\n  --vault-password-file $VAULT_PASSWORD_PATH \\\n  ansible/playbooks/deploy.yml\n</code></pre>"},{"location":"developer-guide/#integration-tests","title":"Integration tests","text":"<p>See integration tests section in ci-cd.md</p>"},{"location":"developer-guide/#install-tkn","title":"Install tkn","text":"<p>You should install the tkn CLI which corresponds to the version of the cluster you're utilizing.</p>"},{"location":"developer-guide/#using-codeready-containers","title":"Using CodeReady Containers","text":"<p>It's possible to deploy and test the pipelines from a CodeReady Containers (CRC) cluster for development/testing, purposes.</p> <ol> <li> <p>Install CodeReady Containers</p> </li> <li> <p>Install OpenShift Pipelines</p> </li> <li> <p>Login to your cluster with <code>oc</code> CLI.</p> <p>You can run <code>crc console --credentials</code> to get the admin login command.</p> </li> <li> <p>Create a test project in your cluster</p> <p><code>bash oc new-project playground</code></p> </li> <li> <p>Grant the <code>privileged</code> SCC to the default <code>pipeline</code> service account.</p> <p>The <code>buildah</code> task requires the <code>privileged</code> security context constraint in order to call <code>newuidmap</code>/<code>newgidmap</code>. This is only necessary because <code>runAsUser:0</code> is defined in <code>templates/crc-pod-template.yml</code>.</p> <p><code>bash oc adm policy add-scc-to-user privileged -z pipeline</code></p> </li> </ol>"},{"location":"developer-guide/#running-a-pipeline-with-crc","title":"Running a Pipeline with CRC","text":"<p>It may be necessary to pass the following <code>tkn</code> CLI arg to avoid permission issues with the default CRC PersistentVolumes.</p> <pre><code>--pod-template templates/crc-pod-template.yml\n</code></pre>"},{"location":"developer-guide/#prepare-a-certification-project","title":"Prepare a Certification Project","text":"<p>A certification project is required for executing all pipelines. In order to avoid collisions with other developers, it's best to create a new one in the corresponding Pyxis environment.</p> <p>The pipelines depend on the following certification project fields:</p> <pre><code>{\n  \"project_status\": \"active\",\n  \"type\": \"Containers\",\n\n  // Arbitrary name for the project - can be almost anything\n  \"name\": \"&lt;insert-project-name&gt;\",\n\n  /*\n   Either \"connect\", \"marketplace\" or \"undistributed\".\n   This maps to the `organization` field in the bundle submission repo's config.yaml.\n     connect -&gt; certified-operators\n     marketplace -&gt; redhat-marketplace\n     undistributed -&gt; certified-operators (certified against, but not distributed to)\n  */\n  \"operator_distribution\": \"&lt;insert-distribution&gt;\",\n\n  // Must correspond to a containerVendor record with the same org_id value.\n  \"org_id\": &lt;insert-org-id&gt;,\n\n  \"container\": {\n    \"type\": \"operator bundle image\",\n\n    \"build_catagories\":\"Operator bundle\",\n\n    // Required but always \"rhcc\"\n    \"distribution_method\": \"rhcc\",\n\n    // Always set to true to satisfy the publishing checklist\n    \"distribution_approval\": true,\n\n    // Must match the github user(s) which opened the test pull requests\n    \"github_usernames\": [\"&lt;insert-github-username&gt;\"],\n\n    // Must be unique for the vendor\n    \"repository_name\": \"&lt;insert-repo-name&gt;\"\n  }\n}\n</code></pre>"},{"location":"developer-guide/#prepare-an-operator-bundle","title":"Prepare an Operator Bundle","text":"<p>You can use a utility script to copy an existing bundle. By default it will copy a bundle that should avoid common failure conditions such as digest pinning. View all the customization options by passing <code>-h</code> to this command.</p> <pre><code>./scripts/copy-bundle.sh\n</code></pre> <p>You may wish to tweak the generated output to influence the behavior of the pipelines. For example, Red Hat Marketplace Operator bundles may require additional annotations. The pipeline should provide sufficient error messages to indicate what is missing. If such errors are unclear, that is likely a bug which should be fixed.</p>"},{"location":"developer-guide/#prepare-your-ciyaml","title":"Prepare Your ci.yaml","text":"<p>At the root of your operator package directory (note: not the bundle version directory) there needs to be a <code>ci.yaml</code> file. For development purposes, it should follow this format in most cases.</p> <pre><code>---\n# Copy this value from the _id field of the certification project in Pyxis.\ncert_project_id: &lt;pyxis-cert-project-id&gt;\n# Set this to true to allow the hosted pipeline to merge pull requests.\nmerge: false\n</code></pre>"},{"location":"developer-guide/#create-a-bundle-pull-request","title":"Create a Bundle Pull Request","text":"<p>It's recommended to open bundle pull requests against the operator-pipelines-test repo. The pipeline GitHub bot account has permissions to manage it.</p> <p>Note: This repository is only configured for testing certified operators, NOT Red Hat Marketplace operators (see <code>config.yaml</code>).</p> <pre><code># Checkout the pipelines test repo\ngit clone https://github.com/redhat-openshift-ecosystem/operator-pipelines-test\ncd operator-pipelines-test\n\n# Create a new branch\ngit checkout -b &lt;insert-branch-name&gt;\n\n# Copy your package directory\ncp -R &lt;package-dir&gt;/ operators/\n\n# Commit changes\ngit add -A\n\n# Use this commit pattern so it defaults to the pull request title.\n# This is critical to the success of the pipelines.\ngit commit -m \"operator &lt;operator-package-name&gt; (&lt;bundle-version&gt;)\"\n\n# Push your branch. Open the pull request using the output.\ngit push origin &lt;insert-branch-name&gt;\n</code></pre> <p>Note: You may need to merge the pull request to use it for testing the release pipeline.</p>"},{"location":"developer-guide/#create-an-api-key","title":"Create an API Key","text":"<p>File a ticket with Pyxis admins to assist with this request. It must correspond to the <code>org_id</code> for the certification project under test.</p>"},{"location":"developer-guide/#making-changes-to-the-pipelines","title":"Making Changes to the Pipelines","text":""},{"location":"developer-guide/#guiding-principles","title":"Guiding Principles","text":"<ul> <li>Avoid including credentials within Task scripts.</li> <li>Avoid the use of <code>set -x</code> in shell scripts which could expose credentials     to the console.</li> <li>Don't use workspaces for passing secrets. Use <code>secretKeyRef</code> and <code>volumeMount</code>   with secret and key names instead.</li> <li>Reason: It adds unnecessary complexity to <code>tkn</code> commands.</li> <li>Use images from trusted registries/namespaces.</li> <li>registry.redhat.io</li> <li>registry.access.redhat.com</li> <li>quay.io/redhat-isv</li> <li>quay.io/opdev</li> <li>Use image pull specs with digests instead of tags wherever possible.</li> <li>Tasks must implement their own skipping behavior, if needed.</li> <li>Reason: If a task is not executed, any dependent tasks will not be     executed either.</li> <li>Don't use ClusterTasks or upstream tasks. All tasks are defined in this repo.</li> <li>Document all params, especially pipeline params.</li> <li>Output human readable logs.</li> <li>Use reasonable defaults for params wherever possible.</li> </ul>"},{"location":"developer-guide/#applying-pipeline-changes","title":"Applying Pipeline Changes","text":"<p>You can use the following command to apply all local changes to your OCP project. It will add all the Tekton resources used across all the pipelines.</p> <pre><code>oc apply -R -f ansible/roles/operator-pipeline/templates/openshift\n</code></pre>"},{"location":"developer-guide/#making-changes-to-the-pipeline-image","title":"Making Changes to the Pipeline Image","text":""},{"location":"developer-guide/#dependency","title":"Dependency","text":"<p>Operator pipelines project is configured to automatically manage Python dependencies using PDM tool. The pdm automates definition, installation, upgrades and the whole lifecycle of dependency in a project. All dependencies are stored in <code>pyproject.toml</code> file in a groups that corresponds to individual applications within the Operator pipelines project.</p> <p>Adding, removing and updating of dependency needs to be always done using <code>pdm</code> cli.</p> <pre><code>pdm add -G operator-pipelines gunicorn==20.1.0\n</code></pre> <p>After a dependency is installed it is added to pdm.lock file. The lock file is always part of git repository.</p> <p>If you want to install specific group set of dependencies use following command:</p> <pre><code>pdm install -G operator-pipelines\n</code></pre> <p>Dependencies are stored into virtual environment (.venv) which is automatically created after <code>pdm install</code>. If .venv wasn't created, configure pdm to automatically create it during installation with <code>pdm config python.use_venv true</code>.</p>"},{"location":"developer-guide/#run-unit-tests-code-style-checkers-etc","title":"Run Unit Tests, Code Style Checkers, etc.","text":"<p>Before running the tests locally, the environment needs to be prepared. Choose the preparation process according to your Linux version.</p>"},{"location":"developer-guide/#preparation-on-rpm-based-linux","title":"Preparation on RPM-based Linux","text":"<pre><code>sudo dnf -y install hadolint\npython3 -m pip install pdm\npdm venv create 3.12\npdm install\nsource .venv/bin/activate\npython3 -m pip install ansible-lint\n</code></pre>"},{"location":"developer-guide/#preparation-on-other-linux-systems","title":"Preparation on other Linux systems","text":"<p>Before starting, make sure you have installed the Brew package manager.</p> <pre><code>brew install hadolint\npython3 -m pip install pdm\npdm venv create 3.12\npdm install\nsource .venv/bin/activate\npython3 -m pip install ansible-lint\n</code></pre>"},{"location":"developer-guide/#run-the-local-tests","title":"Run the local tests","text":"<p>To run unit tests and code style checkers:</p> <pre><code>tox\n</code></pre>"},{"location":"developer-guide/#local-development","title":"Local development","text":"<p>Setup python virtual environment using pdm.</p> <pre><code>pdm venv create 3.12\npdm install\nsource .venv/bin/activate\n</code></pre>"},{"location":"developer-guide/#build-push","title":"Build &amp; Push","text":"<ol> <li> <p>Ensure you have buildah    installed</p> </li> <li> <p>Build the image</p> <p><code>bash buildah bud</code></p> </li> <li> <p>Push the image to a remote registry, eg. Quay.io.</p> <p><code>bash buildah push &lt;image-digest-from-build-step&gt; &lt;remote-repository&gt;</code></p> <p>This step may require login, eg.</p> <p><code>bash buildah login quay.io</code></p> </li> </ol>"},{"location":"fbc-catalog-promotion/","title":"FBC catalog promotion","text":"<p>Every couple of months when OpenShift releases a new version, operators needs to be promoted from the previous version to the new one. In the non-FBC mode this is done at the time of the index image bootstrapping. Based on the annotation <code>com.redhat.openshift.versions</code> the operator is promoted to the next version.</p> <p>In the FBC mode the process is a bit different, as a source of the catalog is stored in the git repository. The process of promoting the operator to the next version is driven by the semi-automated process.</p>"},{"location":"fbc-catalog-promotion/#fbc-catalog-promotion-process","title":"FBC catalog promotion process","text":"<p>Before an Openshift version <code>N</code> is GA and index image <code>N+1</code> is ready, the operator pipeline maintainers needs to prepare the operator for the next version. The automated script will create a pull request with the changes needed to promote the operator to the next version.</p> <p>Based on the FBC configuration in the <code>ci.yaml</code> file, the script will exectute the following steps: - Detect FBC onboarder operators <code>fbc.enabled: true</code> - Detect the <code>fbc.version_promotion_strategy</code> option - Copy catalog from <code>N</code> to <code>N+1</code> within the <code>catalogs</code> directory - Update the <code>catalog_mapping</code> in the <code>ci.yaml</code> file</p> <p>Based on the <code>fbc.version_promotion_strategy</code> option, the script will create a pull request. - If the <code>fbc.version_promotion_strategy</code> is set to <code>review-needed</code>, a one PR per operator will be created and operator owners will be asked for an approval. - If the <code>fbc.version_promotion_strategy</code> is set to <code>always</code>, a signle PR will be created for all operators with the same strategy and PR will be released automatically. - If the <code>fbc.version_promotion_strategy</code> is set to <code>never</code>, no PR will be created and the operator will not be promoted to the next version.</p>"},{"location":"fbc-catalog-promotion/#usage","title":"Usage","text":"<p>Pull the latest git repository of operator-pipelines</p> <pre><code>git clone git@github.com:redhat-openshift-ecosystem/operator-pipelines.git\ncd operator-pipelines\n</code></pre> <p>The script is available in <code>scripts/promote-catalog.py</code>. Before running the script export <code>GITHUB_TOKEN</code> environment variable with a token that has access to the repository.</p> <pre><code>export GITHUB_TOKEN=&lt;token&gt;\n</code></pre> <p>And then run the script. Always use the <code>--dry-run</code> option first to see what will be done.</p> <pre><code>python scripts/promote-catalog.py \\\n    --local-repo /tmp/community-operators-pipeline-preprod \\\n    --target-version 4.19 \\\n    --verbose --dry-run\n</code></pre>"},{"location":"index-signature-verification/","title":"Index Signature Verification","text":"<p>This repository contains a special Tekton pipeline for checking the signature status of the production index images. For now, it is only intended to be deployed manually on a single cluster. The pipeline is regularly scheduled via a CronJob and runs to completion without sending a direct notification upon success or failure. Instead, it relies on other resources to handle reporting.</p> <p>The pipeline should be deployed using Ansible.</p> <pre><code>ansible-playbook \\\n    -i inventory/clusters \\\n    -e \"clusters={INSERT ANSIBLE HOST LIST}\" \\\n    -e \"ocp_token={INSERT TOKEN}\" \\\n    -e \"k8s_validate_certs={yes|no}\" \\\n    --vault-password-file \"{INSERT FILE}\" \\\n    playbooks/deploy-index-signature-verification.yml\n</code></pre>"},{"location":"ocp-namespace-config/","title":"OpenShift namespaces configuration","text":"<p>Operator pipelines are deployed and run in OpenShift Dedicated clusters. The deployment of all resources including pipelines, tasks, secrets and others is managed using Ansible playbooks. In order to be able run the ansible automated way the initial setup of OpenShift namespaces needs to be executed. This process is also automated and requires access to a cluster with the <code>cluster-admin</code> privileges.</p> <p>To initially create and configure namespaces for each environment use following script:</p> <pre><code>cd ansible\n# Store Ansible vault password in ./vault-password\necho $VAULT_PASSWD &gt; ./vault-password\n\n# Login to a cluster using oc\noc login --token=$TOKEN --server=$OCP_SERVER\n\n# Trigger an automation\n./init.sh stage\n</code></pre> <p>This command triggers Ansible that automates creation of OCP namespace for given environment and store admin service account token into vault.</p>"},{"location":"pipeline-admin-guide/","title":"Operator pipeline admin guide","text":"<p>This document aims to provide information needed for maintenance and troubleshooting of operator pipelines.</p>"},{"location":"pipeline-admin-guide/#operator-repositories","title":"Operator repositories","text":"<ul> <li>Certified operators<ul> <li>prod: https://github.com/redhat-openshift-ecosystem/certified-operators</li> <li>nonprod: https://github.com/redhat-openshift-ecosystem/certified-operators-preprod</li> </ul> </li> <li>Marketplace operators<ul> <li>prod: https://github.com/redhat-openshift-ecosystem/redhat-marketplace-operators</li> <li>nonprod: https://github.com/redhat-openshift-ecosystem/redhat-marketplace-operators-preprod</li> </ul> </li> <li>Community OCP operators<ul> <li>prod: https://github.com/redhat-openshift-ecosystem/community-operators-prod</li> <li>nonprod: https://github.com/redhat-openshift-ecosystem/community-operators-pipeline-preprod/</li> </ul> </li> </ul> <p>Pre-production repositories are used for all pre-prod environments (stage, dev, qa). Each environment has a dedicated git branch. By selecting a target branch you can select an environment where the operator will be tested.</p>"},{"location":"pipeline-admin-guide/#ocp-environments","title":"OCP environments","text":"<ul> <li>https://console-openshift-console.apps.pipelines-prod.ijdb.p1.openshiftapps.com/pipelines/ns/operator-pipeline-prod</li> <li>https://console-openshift-console.apps.pipelines-stage.0ce8.p1.openshiftapps.com/pipelines/ns/operator-pipeline-stage</li> </ul>"},{"location":"pipeline-admin-guide/#pipelines","title":"Pipelines","text":"<p>Testing and certification of OpenShift operators from ISV and Community sources is handled by OpenShift Pipelines (Tekton)</p>"},{"location":"pipeline-admin-guide/#isv-pipelines","title":"ISV pipelines","text":"<ul> <li>ISV CI pipeline - The CI pipeline is used by Red Hat Partners to test their operator on their own infrastructure. The main purpose of the pipeline is that the cadence of development cycle can be increased by running the tests locally without a need of submitting the operators to Red Hat. The local infrastructure enables testing operators that have a non trivial HW requirements.</li> <li>ISV Hosted pipeline - The hosted pipeline provides a testing environment and workflow for ISV operators. The pipeline accepts an operator submitted by ISV in Github repository and executes basic linting, static checks and dynamic tests. The pipeline also communicates with Red Hat internal systems and API in order to control a flow or certification. The pipeline success means the operator bundle meets all certification policies and can be distributed to end users.</li> <li>ISV Release pipeline - The release pipeline distributes ISV operator to all index images supported by the operator bundle and make sure the new operator is visible on Red Hat Ecosystem catalog.</li> </ul>"},{"location":"pipeline-admin-guide/#community-pipelines","title":"Community pipelines","text":"<ul> <li>Hosted pipeline - Similarly as the ISV hosted pipeline, the community hosted pipeline verifies if a new operator bundle follows all pre-defined rules and passes all the community tests. The pipeline verifies that operator can be installed on a OCP cluster. A Preflight tools is used to execute a dynamic test suite</li> <li>Release pipeline - The release pipeline distributes a community operators into OCP catalog.</li> </ul>"},{"location":"pipeline-admin-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"pipeline-admin-guide/#pipeline-states","title":"Pipeline states","text":"<p>After an operator is submitted to any of the repositories mentioned above a operator pipeline kicks in. The current state of the pipeline is indicated by the PR labels. Right after a pipeline starts a label <code>operator-hosted-pipeline/started</code> is added. Based on the result of the pipeline one of the following labels is added and <code>*/started</code> label is removed: - <code>operator-hosted-pipeline/passed</code> - <code>operator-hosted-pipeline/failed</code></p> <p>If the hosted pipeline finished successfully and PR has been approved the pipeline merges the PR. The merge event is a trigger for the release pipeline. The release pipeline also applies labels based on the current pipeline status. - <code>operator-release-pipeline/started</code> - <code>operator-release-pipeline/passed</code> - <code>operator-release-pipeline/failed</code></p> <p>In the best case scenario at the end of a process a PR should have both hosted and release <code>*/passed</code> labels.</p> <p></p>"},{"location":"pipeline-admin-guide/#re-trigger-mechanism","title":"Re-trigger mechanism","text":"<p>In case of pipeline failure user or repository owner can re-trigger a pipeline using PR labels. Since the labels can't be set by external contributor a pipeline can be also re-triggered using PR comments. The re-trigger mechanism allows user to re-trigger pipeline only when previous pipeline ended up in failed state.</p> <p>The pipeline summary provides a description of the failure and a hint of how to re-trigger the pipeline.</p> <p>The command that re-triggers a pipeline is in a following format:</p> <p><code>/pipeline restart &lt;pipeline name&gt;</code></p> <p>Based on which pipeline fails one of these command can be used to re-trigger it again:</p> <ul> <li><code>/pipeline restart operator-hosted-pipeline</code></li> <li><code>/pipeline restart operator-release-pipeline</code></li> </ul> <p></p> <p>After a pipeline is re-triggered using the command a few labels will be added and removed from the PR. First a new labels <code>pipeline/trigger-hosted</code> or <code>pipeline/trigger-release</code> is added. This label kick in the pipeline and pipeline itself start adding a labels based on the pipeline status.</p> <p>A script called <code>bulk-retrigger</code> is provided in the operator-pipeline container image to help re-triggering a pipeline on multiple PRs: it takes the repository name, a CSV file containing a list of PRs to process and automates the re-triggering of the pipeline one PR at a time. See the help text for details on how to run it.</p>"},{"location":"pipeline-admin-guide/#pipeline-logs","title":"Pipeline logs","text":"<p>Pipelines interacts with user using a Github Pull request interface. There are a slight differences between ISV and community repositories, but overall concept is the same.</p> <p>At the end of pipeline run a pipeline submits a pipeline summary comment with a basis pipeline metrics and overview of individual tasks.</p> <p>The community pipeline also directly attaches a link to a Github Gist with a pipeline logs. The ISV pipeline uploads logs and artifacts to Pyxis and logs are available to partner through Red Hat Connect.</p> <p></p>"},{"location":"pipeline-admin-guide/#skip-tests","title":"Skip tests","text":"<p>In certain corner cases there is a real need to skip a subset of tests and force a pipeline to pass even though not all checks are green. This is usually initiated by submitting an exception from ISV or community members. In case an exception is reviewed and approved a pipeline has a mechanism to skip selected tests.</p> <p>To to skip a static or dynamic test a repository administrator needs to apply a PR label in the following format:</p> <p><code>tests/skip/&lt;name of the test&gt;</code></p> <p>So for example if case an operator can't be installed with a default settings and requires a special environment we can skip <code>DeployableByOLM</code> by adding <code>tests/skip/DeployableByOLM</code> label to a PR.</p>"},{"location":"pipeline-env-setup/","title":"Pipeline Environment Setup","text":"<p>Common for all the pipelines</p> <p>Only CI Pipeline</p> <p>Only Hosted Pipeline</p> <p>Only Release Pipeline</p>"},{"location":"pipeline-env-setup/#common-for-all-the-pipelines","title":"Common for all the pipelines:","text":""},{"location":"pipeline-env-setup/#red-hat-catalog-imagestreams","title":"Red Hat Catalog Imagestreams","text":"<p>The pipelines must pull the parent index images through the internal OpenShift registry to take advantage of the built-in credentials for Red Hat's terms-based registry (registry.redhat.io). This saves the user from needing to provide such credentials. The index generation task will always pull published index images through imagestreams of the same name in the current namespace. As a result, there is a one time configuration for each desired distribution catalog. Replace the <code>from</code> argument when configuring this for pre-production environments.</p> <pre><code># Must be run once before certifying against the certified catalog.\noc --request-timeout 10m import-image certified-operator-index \\\n  --from=registry.redhat.io/redhat/certified-operator-index \\\n  --reference-policy local \\\n  --scheduled \\\n  --confirm \\\n  --all\n\n# Must be run once before certifying against the Red Hat Marketplace catalog.\noc --request-timeout 10m import-image redhat-marketplace-index \\\n  --from=registry.redhat.io/redhat/redhat-marketplace-index \\\n  --reference-policy local \\\n  --scheduled \\\n  --confirm \\\n  --all\n</code></pre>"},{"location":"pipeline-env-setup/#only-ci-pipeline","title":"Only CI pipeline:","text":""},{"location":"pipeline-env-setup/#registry-credentials","title":"Registry Credentials","text":"<p>The CI pipeline can optionally be configured to push and pull images to/from a remote private registry. The user must create an auth secret containing the docker config. This secret can then be passed as a workspace named <code>registry-credentials</code> when invoking the pipeline.</p> <pre><code>oc create secret generic registry-dockerconfig-secret \\\n  --type kubernetes.io/dockerconfigjson \\\n  --from-file .dockerconfigjson=config.json\n</code></pre>"},{"location":"pipeline-env-setup/#git-ssh-secret","title":"Git SSH Secret","text":"<p>The pipelines requires git SSH credentials with write access to the repository if automatic digest pinning is enabled using the pin_digests param. This is disabled by default. Before executing the pipeline the user must create a secret in the same namespace as the pipeline.</p> <p>To create the secret run the following commands (substituting your key):</p> <pre><code>cat &lt;&lt; EOF &gt; ssh-secret.yml\nkind: Secret\napiVersion: v1\nmetadata:\n  name: github-ssh-credentials\ndata:\n  id_rsa: |\n    &lt; PRIVATE SSH KEY &gt;\nEOF\n\noc create -f ssh-secret.yml\n</code></pre>"},{"location":"pipeline-env-setup/#container-api-access","title":"Container API access","text":"<p>CI pipelines automatically upload a test results, logs and artifacts using Red Hat container API. This requires a partner's API key and the key needs to be created as a secret in OpenShift cluster before running a Tekton pipeline.</p> <pre><code>oc create secret generic pyxis-api-secret --from-literal pyxis_api_key=&lt; API KEY &gt;\n</code></pre>"},{"location":"pipeline-env-setup/#kubeconfig","title":"Kubeconfig","text":"<p>The CI pipeline requires a kubeconfig with admin credentials. This can be created by logging into said cluster as an admin user.</p> <pre><code>KUBECONFIG=kubeconfig oc login -u &lt;username&gt; -p &lt;password&gt;\noc create secret generic kubeconfig --from-file=kubeconfig=kubeconfig\n</code></pre>"},{"location":"pipeline-env-setup/#github-api-token","title":"GitHub API token","text":"<p>To automatically open the PR with submission, pipeline must authenticate to GitHub. Secret containing api token should be created.</p> <pre><code>oc create secret generic github-api-token --from-literal GITHUB_TOKEN=&lt; GITHUB TOKEN &gt;\n</code></pre>"},{"location":"pipeline-env-setup/#only-hosted-pipeline","title":"Only Hosted pipeline:","text":""},{"location":"pipeline-env-setup/#registry-credentials_1","title":"Registry Credentials","text":"<p>The hosted pipeline requires credentials to push/pull bundle and index images from a pre-release registry (quay.io). A registry auth secret must be created. This secret can then be passed as a workspace named <code>registry-credentials</code> when invoking the pipeline.</p> <pre><code>oc create secret generic hosted-pipeline-registry-auth-secret \\\n  --type kubernetes.io/dockerconfigjson \\\n  --from-file .dockerconfigjson=config.json\n</code></pre>"},{"location":"pipeline-env-setup/#container-api-access_1","title":"Container API access","text":"<p>The hosted pipeline communicates with internal Container API that requires cert + key. The corresponding secret needs to be created before running the pipeline.</p> <pre><code>oc create secret generic operator-pipeline-api-certs \\\n  --from-file operator-pipeline.pem \\\n  --from-file operator-pipeline.key\n</code></pre>"},{"location":"pipeline-env-setup/#hydra-credentials","title":"Hydra credentials","text":"<p>To verify publishing checklist, Hosted pipeline uses Hydra API. To authenticate with Hydra over basic auth, secret containing service account credentials should be created.</p> <pre><code>oc create secret generic hydra-credentials \\\n  --from-literal username=&lt;username&gt;  \\\n  --from-literal password=&lt;password&gt;\n</code></pre>"},{"location":"pipeline-env-setup/#github-bot-token","title":"GitHub Bot token","text":"<p>To automatically merge the PR, Hosted pipeline uses GitHub API. To authenticate when using this method, secret containing bot token should be created.</p> <pre><code>oc create secret generic github-bot-token --from-literal github_bot_token=&lt; BOT TOKEN &gt;\n</code></pre>"},{"location":"pipeline-env-setup/#prow-kubeconfig","title":"Prow-kubeconfig","text":"<p>Hosted preflight tests are run on the separate cluster. To provision a cluster destined for the tests, the pipeline uses a Prowjob. Thus, to start the preflight test, there needs to be a prow-specific kubeconfig.</p> <ul> <li>ProwJob</li> <li>OperatorCI</li> </ul> <pre><code>oc create secret generic prow-kubeconfig \\\n  --from-literal kubeconfig=&lt;kubeconfig&gt;\n</code></pre>"},{"location":"pipeline-env-setup/#preflight-decryption-key","title":"Preflight decryption key","text":"<p>Results of the preflight tests are protected by encryption. In order to retrieve them from the preflight job, gpg decryption key should be supplied.</p> <pre><code>oc create secret generic preflight-decryption-key \\\n  --from-literal private=&lt;private gpg key&gt; \\\n  --from-literal public=&lt;public gpg key&gt;\n</code></pre>"},{"location":"pipeline-env-setup/#quay-oauth-token","title":"Quay OAuth Token","text":"<p>A Quay OAuth token is required to set repo visibility to public.</p> <pre><code>oc create secret generic quay-oauth-token --from-literal token=&lt;token&gt;\n</code></pre>"},{"location":"pipeline-env-setup/#only-release-pipeline","title":"Only Release pipeline:","text":""},{"location":"pipeline-env-setup/#registry-credentials_2","title":"Registry Credentials","text":"<p>The release pipeline requires credentials to push and pull the bundle image built by the hosted pipeline. Three registry auth secrets must be specified since different credentials may be required for the same registry when copying and serving the image. These secrets can then be passed as workspaces named <code>registry-pull-credentials</code>, <code>registry-push-credentials</code> and <code>registry-serve-credentials</code> when invoking the pipeline.</p> <pre><code>oc create secret generic release-pipeline-registry-auth-pull-secret \\\n  --type kubernetes.io/dockerconfigjson \\\n  --from-file .dockerconfigjson=pull-config.json\n\noc create secret generic release-pipeline-registry-auth-push-secret \\\n  --type kubernetes.io/dockerconfigjson \\\n  --from-file .dockerconfigjson=push-config.json\n\noc create secret generic release-pipeline-registry-auth-serve-secret \\\n  --type kubernetes.io/dockerconfigjson \\\n  --from-file .dockerconfigjson=serve-config.json\n</code></pre>"},{"location":"pipeline-env-setup/#kerberos-credentials","title":"Kerberos credentials","text":"<p>For submitting the IIB build, you need kerberos keytab in a secret:</p> <pre><code>oc create secret generic kerberos-keytab \\\n  --from-file krb5.keytab\n</code></pre>"},{"location":"pipeline-env-setup/#quay-credentials","title":"Quay credentials","text":"<p>Release pipeline uses Quay credentials to authenticate a push to an index image during the IIB build.</p> <pre><code>oc create secret generic iib-quay-credentials \\\n  --from-literal username=&lt;QUAY_USERNAME&gt; \\\n  --from-literal password=&lt;QUAY_PASSWORD&gt;\n</code></pre>"},{"location":"pipeline-env-setup/#ocp-registry-kubeconfig","title":"OCP-registry-kubeconfig","text":"<p>OCP clusters contains the public registries for Operator Bundle Images. To publish the image to this registry, Pipeline connects to OCP cluster via kubeconfig. To create the secret which contains the OCP cluster kubeconfig:</p> <pre><code>oc create secret generic ocp-registry-kubeconfig \\\n  --from-literal kubeconfig=&lt;kubeconfig&gt;\n</code></pre> <p>Additional setup instructions for this cluster are documented here.</p>"},{"location":"preflight-invalidation/","title":"Preflight invalidation CronJob","text":"<p>The repository contains a CronJob that updates enabled preflight versions weekly. https://issues.redhat.com/browse/ISV-4964</p> <p>After changes, the CronJob can be deployed using Ansible.</p> <pre><code>ansible-playbook \\\n    -i ansible/inventory/clusters \\\n    -e \"clusters=prod-cluster\" \\\n    -e \"ocp_token=[TOKEN]\" \\\n    -e \"env=prod\" \\\n    --vault-password-file [PWD_FILE] \\\n    playbooks/preflight-invalidation.yml\n</code></pre>"},{"location":"release-and-rollback/","title":"Release Schedule","text":"<p>Every Monday and Wednesday, except for hotfixes.</p>"},{"location":"release-and-rollback/#hotfixes","title":"Hotfixes","text":"<p>Hotfixes are defined as changes that need to be quickly deployed to prod, outside of the regular release schedule, to address major issues that occur in prod. Hotfixes should still follow the release criteria and process, and should be announced on the team chat so that the rest of the team is aware.</p>"},{"location":"release-and-rollback/#release-criteria","title":"Release Criteria","text":"<ul> <li>Change is submitted as a pull request on Github.</li> <li>All checks (validations and tests) pass on the pull request.</li> <li>Pull request is reviewed and approved by at least one other ISV Guild member.</li> <li>Change is merged into main branch.</li> <li>A new release must be deployed to dev and qa before being deployed to stage</li> <li>A new release must be deployed to stage in the previous scheduled release   date before being deployed to prod</li> <li>Stage and prod deployments are manually triggered by approving     the related Github Actions.</li> </ul>"},{"location":"release-and-rollback/#release-process","title":"Release Process","text":"<p>Before deployments occur, a new container image will be built and with \u201clatest\u201d and the associated git commit sha, then pushed to quay.io.</p> <p>Dev and qa deployment will happen automatically by Github Actions every time a change is merged into the main branch. The commit sha will be passed for identify the container image used by the pipelines as part of deployments.</p> <p>During a scheduled release or hotfix, stage and prod deployment will only happen by manually triggering the \u201cdeploy-stage\u201d and \u201cdeploy-prod\u201d Github Actions respectively. In a scheduled release, changes that were previously deployed to dev and qa will be promoted to stage, and changes that were previously deployed to stage will be promoted to prod. The last container image used in dev and qa (identified by the git commit sha tag) will also be promoted to be used in the stage pipeline, while the container image last used in stage will be used in the prod pipeline.</p>"},{"location":"release-and-rollback/#rollback-process","title":"Rollback Process","text":""},{"location":"release-and-rollback/#short-term-rollbacks","title":"Short term rollbacks","text":"<p>For short term rollbacks: Re-run deployment from a previous stable release. Since the container image is identified by the git commit sha, re-running a previous deployment will also roll back the container image that\u2019s used to a previous one.</p>"},{"location":"release-and-rollback/#longer-term-rollbacks","title":"Longer term rollbacks","text":"<p>Revert commit(s) that need to be rolled back, then follow the regular release process to deploy.</p>"},{"location":"users/best-practices/","title":"Operator Best Practices","text":"<p>Check the sections Best Practices for OLM and SDK projects to know more about its best practices and common recommendations, suggestions and conventions, see:</p> <ul> <li>SDK best practices</li> <li>OLM best practices</li> </ul>"},{"location":"users/community-operators-troubleshooting/","title":"Troubleshooting the Community Operator Pipeline","text":"<p>This document provides troubleshooting steps for each Tekton task in the Operator Hosted Pipeline and Operator Release Pipeline, including tasks for FBC-enabled operators in the community-operators-prod repository.</p>"},{"location":"users/community-operators-troubleshooting/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Troubleshooting the Community Operator Pipeline</li> <li>Table of Contents</li> <li>Operator Pipeline Tasks Troubleshooting<ul> <li>get-pr-number</li> <li>acquire-lease</li> <li>set-github-started-label</li> <li>set-github-status-pending</li> <li>set-env</li> <li>clone-repository-base</li> <li>clone-repository</li> <li>detect-changes</li> <li>yaml-lint</li> <li>check-permissions</li> <li>set-github-pr-title</li> <li>read-config</li> <li>resolve-pr-type</li> <li>apply-test-waivers</li> <li>content-hash</li> <li>certification-project-check</li> <li>get-organization</li> <li>get-pyxis-certification-data</li> <li>static-tests</li> <li>static-tests-results</li> <li>merge-registry-credentials</li> <li>digest-pinning</li> <li>verify-pinned-digest</li> <li>dockerfile-creation</li> <li>build-bundle</li> <li>make-bundle-repo-public</li> <li>get-supported-versions</li> <li>add-bundle-to-index</li> <li>make-index-repo-public</li> <li>get-ci-results-attempt</li> <li>preflight-trigger</li> <li>evaluate-preflight-result</li> <li>get-ci-results</li> <li>link-pull-request-with-open-status</li> <li>merge-pr</li> <li>link-pull-request-with-merged-status</li> <li>copy-bundle-image-to-released-registry</li> <li>decide-index-paths</li> <li>get-manifest-digests</li> <li>request-signature</li> <li>upload-signature</li> <li>publish-to-index</li> </ul> </li> <li>FBC-Related Operator Pipeline Tasks Troubleshooting<ul> <li>build-fbc-index-images</li> <li>build-fbc-scratch-catalog</li> </ul> </li> </ul>"},{"location":"users/community-operators-troubleshooting/#operator-pipeline-tasks-troubleshooting","title":"Operator Pipeline Tasks Troubleshooting","text":""},{"location":"users/community-operators-troubleshooting/#get-pr-number","title":"get-pr-number","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#acquire-lease","title":"acquire-lease","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#set-github-started-label","title":"set-github-started-label","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#set-github-status-pending","title":"set-github-status-pending","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#set-env","title":"set-env","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#clone-repository-base","title":"clone-repository-base","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#clone-repository","title":"clone-repository","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#detect-changes","title":"detect-changes","text":"<p>The pipeline may fail at this stage due to the following reasons:</p> <ol> <li>Changing Non-Operator Files: If the PR attempts to modify external files outside of targeted operator, the pipeline will fail.</li> <li>Affecting Multiple Operator Bundles: If the PR impacts more than one operator bundle, it will result in a failure. Update of multiple operators non-bundle files is allowed.</li> <li>Modifying Existing Bundles: Changes to existing bundles in the PR are not allowed at this stage.</li> <li>Deleting Existing Bundles: Deleting bundles is only permissible for FBC-enabled operators.</li> </ol> <p>Other Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#yaml-lint","title":"yaml-lint","text":"<p>Warnings at this step should be addressed if possible but won't result in a failure. Errors at this step will need to be addressed.  Often errors center around unexpected whitespace at the end of lines or missing newlines at the end of your <code>yaml</code> files.</p>"},{"location":"users/community-operators-troubleshooting/#check-permissions","title":"check-permissions","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#set-github-pr-title","title":"set-github-pr-title","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#read-config","title":"read-config","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#resolve-pr-type","title":"resolve-pr-type","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#apply-test-waivers","title":"apply-test-waivers","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#content-hash","title":"content-hash","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#certification-project-check","title":"certification-project-check","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#get-organization","title":"get-organization","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#get-pyxis-certification-data","title":"get-pyxis-certification-data","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#static-tests","title":"static-tests","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#static-tests-results","title":"static-tests-results","text":"<p>If the static tests fail, a summary will be posted as a comment on the Pull Request, providing detailed reasons for the failure. The summary of test results will look like as shown below:</p> <p></p> <p>To proceed:</p> <ol> <li>Review the comment for the detailed reasons behind the failed static tests.</li> <li>Fix all the reported issues.</li> <li>Commit the changes with a fix to the PR and it will Re-trigger the hosted pipeline.</li> </ol> <p>For more information about static tests, refer to the documentation.</p>"},{"location":"users/community-operators-troubleshooting/#merge-registry-credentials","title":"merge-registry-credentials","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#digest-pinning","title":"digest-pinning","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#verify-pinned-digest","title":"verify-pinned-digest","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#dockerfile-creation","title":"dockerfile-creation","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#build-bundle","title":"build-bundle","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#make-bundle-repo-public","title":"make-bundle-repo-public","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#get-supported-versions","title":"get-supported-versions","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#add-bundle-to-index","title":"add-bundle-to-index","text":"<p>Failures at this stage are rare and often due to transient issues. Start by reviewing the pipeline logs linked in the PipelineRun summary within the PR.</p> <p>As an initial step, re-trigger the pipeline by adding the appropriate command in the PR comment:</p> <ol> <li><code>/pipeline restart operator-hosted-pipeline</code> for the hosted pipeline.</li> <li><code>/pipeline restart operator-release-pipeline</code> for the release pipeline.</li> </ol> <p>If the PR fails again after two consecutive attempts, feel free to request assistance in the PR comments. Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#make-index-repo-public","title":"make-index-repo-public","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#get-ci-results-attempt","title":"get-ci-results-attempt","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#preflight-trigger","title":"preflight-trigger","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#evaluate-preflight-result","title":"evaluate-preflight-result","text":"<p>At this step, the pipeline will primarily fail if the dynamic tests do not pass completely. A link to the test artifacts will be posted as a comment on the PR, as shown below.</p> <p></p> <p>Please review this link, as it will provide detailed error information. Failures at this stage are uncommon. To diagnose the issue:</p> <ol> <li>Review the pipeline logs linked in the PipelineRun summary within the PR.</li> <li>Examine the test artifacts for detailed error information.</li> </ol> <p>If the logs and artifacts do not clarify the issue, feel free to ask for assistance in the PR comments. Maintainers will help identify and resolve the problem.</p>"},{"location":"users/community-operators-troubleshooting/#get-ci-results","title":"get-ci-results","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#link-pull-request-with-open-status","title":"link-pull-request-with-open-status","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#merge-pr","title":"merge-pr","text":"<p>If operator hosted pipeline fails at this task with the error message: <code>Pull request Auto merge is not allowed for this repository (enablePullRequestAutoMerge)</code> then re-trigger the pipeline by running command <code>/pipeline restart operator-hosted-pipeline</code>.</p> <p>Another Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#link-pull-request-with-merged-status","title":"link-pull-request-with-merged-status","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#copy-bundle-image-to-released-registry","title":"copy-bundle-image-to-released-registry","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#decide-index-paths","title":"decide-index-paths","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#get-manifest-digests","title":"get-manifest-digests","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#request-signature","title":"request-signature","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#upload-signature","title":"upload-signature","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#publish-to-index","title":"publish-to-index","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#fbc-related-operator-pipeline-tasks-troubleshooting","title":"FBC-Related Operator Pipeline Tasks Troubleshooting","text":""},{"location":"users/community-operators-troubleshooting/#build-fbc-index-images","title":"build-fbc-index-images","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/community-operators-troubleshooting/#build-fbc-scratch-catalog","title":"build-fbc-scratch-catalog","text":"<p>Failures at this stage are rare. To diagnose the issue, review the pipeline logs linked in the PipelineRun summary within the PR. If the logs don\u2019t clarify the problem, feel free to ask for assistance in the PR comments.  Maintainers will assist in identifying and resolving the issue.</p>"},{"location":"users/contributing-prerequisites/","title":"Before submitting your Operator","text":"<p>Important: \"First off, thanks for taking the time to contribute your Operator!\"</p>"},{"location":"users/contributing-prerequisites/#a-primer-to-openshift-community-operators","title":"A primer to Openshift Community Operators","text":"<p>This project collects Community Operators that work with OpenShift to be displayed in the embedded OperatorHub. If you are new to Operators, start here.</p>"},{"location":"users/contributing-prerequisites/#sign-your-work","title":"Sign Your Work","text":"<p>The contribution process works off standard git Pull Requests. Every PR needs to be signed. The sign-off is a simple line at the end of the explanation for a commit. Your signature certifies that you wrote the patch or otherwise have the right to contribute the material. The rules are pretty simple if you can certify the below (from developercertificate.org):</p> <pre><code>Developer Certificate of Origin\nVersion 1.1\n\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\n1 Letterman Drive\nSuite D4700\nSan Francisco, CA, 94129\n\nEveryone is permitted to copy and distribute verbatim copies of this\nlicense document, but changing it is not allowed.\n\n\nDeveloper's Certificate of Origin 1.1\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I\n    have the right to submit it under the open source license\n    indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best\n    of my knowledge, is covered under an appropriate open source\n    license and I have the right under that license to submit that\n    work with modifications, whether created in whole or in part\n    by me, under the same open source license (unless I am\n    permitted to submit under a different license), as indicated\n    in the file; or\n\n(c) The contribution was provided directly to me by some other\n    person who certified (a), (b) or (c) and I have not modified\n    it.\n\n(d) I understand and agree that this project and the contribution\n    are public and that a record of the contribution (including all\n    personal information I submit with it, including my sign-off) is\n    maintained indefinitely and may be redistributed consistent with\n    this project or the open source license(s) involved.\n</code></pre> <p>Then you just add a line to every git commit message:</p> <pre><code>Signed-off-by: John Doe &lt;john.doe@example.com&gt;\n</code></pre> <p>Use your real name (sorry, no pseudonyms or anonymous contributions.)</p> <p>If you set your <code>user.name</code> and <code>user.email</code> git configs, you can sign your commit automatically with <code>git commit -s</code>.</p> <p>Note: If your git config information is set properly then viewing the <code>git log</code> information for your commit will look something like this:</p> <pre><code>Author: John Doe &lt;john.doe@example.com&gt;\nDate:   Mon Oct 21 12:23:17 2019 -0800\n\n    Update README\n\n    Signed-off-by: John Doe &lt;john.doe@example.com&gt;\n</code></pre> <p>Notice the <code>Author</code> and <code>Signed-off-by</code> lines must match.</p>"},{"location":"users/contributing-via-pr/","title":"Submitting your Operator via Pull Requests (PR)","text":""},{"location":"users/contributing-via-pr/#overview","title":"Overview","text":"<p>To submit an operator one has to do these steps</p> <ol> <li>Fork project based on desired Operator Repository</li> <li>Place the operator in the target directory. More info<ul> <li>operators</li> </ul> </li> <li>Configure <code>ci.yaml</code> file. More info<ul> <li>Setup reviewers</li> <li>Enable FBC mode</li> <li>Add template to catalog mapping</li> </ul> </li> <li>Configure the <code>release-config.yaml</code> file if you want to automatically release the operator to the OCP catalogs. More info</li> <li>Make a pull request with a new operator bundle or catalog changes</li> <li>Verify tests and fix problems, if possible</li> <li>Ask for help in the PR in case of problems</li> </ol>"},{"location":"users/contributing-via-pr/#pull-request","title":"Pull request","text":"<p>When a pull request is created, a number of tests are executed via community hosted pipeline. One can see the results in the comment section of conversation tab.</p> <p></p>"},{"location":"users/contributing-via-pr/#you-are-done","title":"You are done","text":"<p>User is done when all tests are green. When the PR is merged, the community release pipeline will be triggered.</p>"},{"location":"users/contributing-via-pr/#test-results-failed","title":"Test results failed?","text":"<p>When operator tests are failing, one can see a following picture</p> <p></p> <p>In case of failures, please have a look at the logs of specific tests. If an error is not clear to you, please ask in the PR. Maintainers will be happy to help you with it.</p>"},{"location":"users/contributing-via-pr/#useful-commands-interacting-with-the-pipeline","title":"Useful commands interacting with the pipeline","text":"<p>You can post the following comment/command:</p> Command Functionality <code>/pipeline restart operator-hosted-pipeline</code> The hosted pipeline will be re-triggered and PR will be merged if possible. The command only works if a previous pipeline failed. <code>/pipeline restart operator-release-pipeline</code> The release pipeline will be re-triggered. The command only works if a previous pipeline failed. <code>/test skip {test_case_name}</code> test_case_name test will be skipped. Please consider that only a subset of tests (currently only pruned graph test) can be skipped."},{"location":"users/contributing-where-to/","title":"Where to contribute","text":"<p>Once you have forked the upstream repo, you will require to add your Operator Bundle to the forked repo. The forked repo will have directory structure similar to the structure outlined below.</p> <pre><code>\u251c\u2500\u2500 config.yaml\n\u251c\u2500\u2500 operators\n\u2502   \u2514\u2500\u2500 new-operator\n\u2502       \u251c\u2500\u2500 0.0.102\n\u2502       \u2502   \u251c\u2500\u2500 manifests\n\u2502       \u2502   \u2502   \u251c\u2500\u2500 new-operator.clusterserviceversion.yaml\n\u2502       \u2502   \u2502   \u251c\u2500\u2500 new-operator-controller-manager-metrics-service_v1_service.yaml\n\u2502       \u2502   \u2502   \u251c\u2500\u2500 new-operator-manager-config_v1_configmap.yaml\n\u2502       \u2502   \u2502   \u251c\u2500\u2500 new-operator-metrics-reader_rbac.authorization.k8s.io_v1_clusterrole.yaml\n\u2502       \u2502   \u2502   \u2514\u2500\u2500 tools.opdev.io_demoresources.yaml\n\u2502       \u2502   \u251c\u2500\u2500 metadata\n\u2502       \u2502   \u2502   \u2514\u2500\u2500 annotations.yaml\n\u2502       \u2502   \u251c\u2500\u2500 release-config.yaml\n\u2502       \u2502   \u2514\u2500\u2500 tests\n\u2502       \u2502       \u2514\u2500\u2500 scorecard\n\u2502       \u2502           \u2514\u2500\u2500 config.yaml\n\u2502       \u251c\u2500\u2500 catalog-templates\n\u2502       \u2502   \u251c\u2500\u2500 v4.14.yaml\n\u2502       \u2502   \u251c\u2500\u2500 v4.15.yaml\n\u2502       \u2502   \u2514\u2500\u2500 v4.16.yaml\n\u2502       \u251c\u2500\u2500 ci.yaml\n\u2502       \u2514\u2500\u2500 Makefile\n\u2514\u2500\u2500 README.md\n</code></pre> <p>Follow the <code>operators</code> directory in the forked repo. Add your Operator Bundle under this <code>operators</code> directory following the example format.</p> <ol> <li>Under the <code>operators</code> directory, create a new directory with the name of your operator.</li> <li>Inside of this newly created directory add your <code>ci.yaml</code> and set its content based on doc.</li> <li>Also, under the new directory create a subdirectory for each version of your Operator.</li> <li>In each version directory there should be a <code>manifests/</code> directory containing your OpenShift yaml files, a <code>metadata/</code> directory containing your <code>annotations.yaml</code> file, and a <code>tests/</code> directory containing the required <code>config.yaml</code> file for the preflight tests.</li> <li>Create a <code>catalog-templates/</code> directory under the operator directory and add a yaml file for each OpenShift version you want to support. The yaml file should contain the catalog template for the operator. More information on how to create the catalog template can be found here.</li> <li>Download the template <code>Makefile</code> from here and place it in the root of the operator directory.</li> </ol> <p>Note To learn more about preflight tests please follow this link.</p> <p>For partners and ISVs, certified operators can now be submitted via connect.redhat.com. If you have submitted your Operator there already, please ensure your submission here uses a different package name (refer to the README for more details).</p>"},{"location":"users/dynamic_checks/","title":"Dynamic checks","text":"<p>The preflight tests are designed to test and verify the the operator bundle content and the format of the operator bundle and if a bundle can be installed on OCP cluster.</p> <p>The result link for the logs of the preflight test runs will be posted to the PR as shown below.</p> <p></p> <p>In case of failures, please have a look at the logs of specific tests. If an error is not clear to you, please ask in the PR. Maintainers will be happy to help you with it.</p> <p>Once all of the tests will be passed successfully, the PR will be merged automatically based on the conditions are met by operator-hosted-pipeline.</p> <p>The PR will not merge automatically in the following cases:</p> <ul> <li>If the brand-new operator is submitted.</li> <li>If the author of the PR is not listed as a reviewer in the <code>ci.yaml</code> file for the respective operator or as a repository maintainer (community only).</li> <li>If the author of the PR is not listed as a reviewer in Red Hat Connect (ISV only)</li> </ul> <p>If there are any of the above cases, the PR needs to be reviewed by the Repositoy maintainers or authorized reviewers. After the approval, the PR needs to be merged manually. Once the PR will be merged, the operator-release-pipeline will be triggered automatically.</p> <p>NOTE: The operator hosted pipeline run results will be posted in the github PR comment.</p> <p></p>"},{"location":"users/fbc_autorelease/","title":"File-Based Catalog - auto-release","text":"<p>By the nature of the File-Based Catalog (FBC) mode, the release of operator is made of two steps.</p> <ul> <li>The first step builds, tests and releases bundle image</li> <li>The second step adds a bundle to OCP catalog and releases it</li> </ul> <p>The second step can be now automated and user is no longer required to manually create a second PR with catalog changes. The release pipeline will take care of it.</p> <p>The process will require an additional configuration in the <code>release-config.yaml</code> file. Once a PR with new bundle and <code>release-config.yaml</code> is merged, the release pipeline will open a new PR with catalog changes.</p> <p>Example of such PR can be found here. The second PR is linked with it original PR and looks like this:</p> <p></p>"},{"location":"users/fbc_autorelease/#release-configyaml","title":"release-config.yaml","text":"<p>If you want your operators to be automatically released to the OCP catalogs in the FBC mode, you will need to configure the <code>release-config.yaml</code> file. The file should be placed into the bundle version directory, e.g. <code>operators/aqua/0.0.2/release-config.yaml</code>.</p> <pre><code>tree operators/aqua\n.\n\u251c\u2500\u2500 0.0.2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 release-config.yaml # This is the file\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 manifests\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 metadata\n\u251c\u2500\u2500 catalog-templates\n\u251c\u2500\u2500 ci.yaml\n\u2514\u2500\u2500 Makefile\n</code></pre> <p>Its content determines where exactly the bundle will be released in terms of the OCP version and the place in the update graph.</p>"},{"location":"users/fbc_autorelease/#example","title":"Example","text":"<pre><code>---\ncatalog_templates:\n  - template_name: basic.yaml\n    channels: [my-channel]\n    replaces: aqua.0.0.1\n  - template_name: semver.yaml\n    channels: [Fast, Stable]\n</code></pre> <p>The example above shows a release configuration where operator bundle is going to be released to the <code>my-channel</code> channel in the <code>basic.yaml</code> catalog template and to the <code>Fast</code> and <code>Stable</code> channels in the <code>semver.yaml</code> catalog template.</p> <p>The <code>replaces</code> field is optional and it specifies the bundle that the new bundle replaces in the update graph.</p>"},{"location":"users/fbc_autorelease/#file-structure","title":"File structure","text":"<p>The schema of the file is available here: release-config.yaml schema. The schema is validated automatically in the pipeline and the PR will fail with explanations if the file is not valid.</p> <p>Here is a summary of the file structure:</p> <ul> <li>The top-level key is <code>catalog_templates</code> which is a list of objects.</li> <li>Each object has the following keys:<ul> <li><code>template_name</code> - the name of the catalog template file in the <code>catalog-templates</code> directory.</li> <li><code>channels</code> - a list of channels where the bundle should be released.<ul> <li>In case of using <code>SemVer</code> a user can pick from allowed values: <code>Fast</code>, <code>Stable</code> and <code>Candidate</code>.</li> </ul> </li> <li><code>replaces</code> - the bundle that the new bundle replaces in the update graph. (Optional, only for the basic templates)</li> <li><code>skips</code> - a list of bundles that should be skipped in the update graph. (Optional, only for the basic templates)</li> <li><code>skipRange</code> - a range of bundles that should be skipped in the update graph. (Optional, only for the basic templates)</li> </ul> </li> </ul>"},{"location":"users/fbc_onboarding/","title":"File Based Catalog onboarding","text":"<p>Note: The File Based Catalog support is now in an alpha phase. We welcome any feedback you have for this new feature.</p> <p>Operators in certified, marketplace, or community repositories are defined in a declarative way. This means a user provides all necessary information in advance about the operator bundle and how it should be released in a catalog and OPM automation injects a bundle into the correct place in the upgrade path.</p> <p>This is however very limited solution that doesn't allow any further modification of upgrade paths after a bundle is already released. Due to this limitation, a concept of FBC (File-based catalog) is now available and allows users to modify the operator upgrade path in a separate step without the need to release a new bundle.</p> <p>To enable FBC for a given operator the operator owner needs to convert existing operator into FBC format.</p> <p>We want to help with this process and we prepared a tooling that helps with this transition.</p>"},{"location":"users/fbc_onboarding/#convert-existing-operator-to-fbc","title":"Convert existing operator to FBC","text":"<p>As a prerequisite to this process, you need to download a <code>Makefile</code> that automates the migration process.</p> <p>An initial system requirement is to have following dependencies installed:  - podman  - make</p> <pre><code># Go to the operator repo directory (certified-operators, marketplace-operators, community-operators-prod)\ncd &lt;operator-repo&gt;/operators/&lt;operator-name&gt;\nwget https://raw.githubusercontent.com/redhat-openshift-ecosystem/operator-pipelines/main/fbc/Makefile\n</code></pre> <p>Now we can convert existing operator into FBC. The initial run takes a while because a local cache is generated during a run.</p> <p>Note A user executing the conversion script needs to be authenticated to registries used by OLM catalog. Use <code>podman login</code> to log in into all registries. A conversion script assumes you have <code>$(XDG_RUNTIME_DIR)/containers/auth.json</code> or <code>~/.docker/config.json</code> present with valid registry tokens.</p> <p>To convert existing operator to <code>FBC</code> format you need to execute following command:</p> <pre><code>$ make fbc-onboarding\n\n2024-04-24 15:53:05,537 [operator-cert] INFO Generating FBC templates for the following versions: ['4.12', '4.13', '4.14', '4.15', '4.16']\n2024-04-24 15:53:07,632 [operator-cert] INFO Processing catalog: v4.12\n2024-04-24 15:53:07,633 [operator-cert] DEBUG Building cache for registry.redhat.io/redhat/community-operator-index:v4.12\n...\n</code></pre> <p>[!IMPORTANT] In case an operator isn't shipped to all OCP catalog versions manually update <code>OCP_VERSIONS</code> variable in the <code>Makefile</code> and include only versions supported by an operator.</p> <p>The Makefile will execute following steps:</p> <ul> <li>Download dependencies needed for the migration (opm, fbc-onboarding CLI)</li> <li>Fetch a list of currently supported OCP catalogs (this might take a while when doing it for the first time)</li> <li>Transform existing catalogs into a basic template</li> <li>Generate an FBC catalog for a given operator</li> <li>Update operator ci.yaml config</li> </ul> <p>After a script is finished you should see a template and generated fbc in the repository.</p> <pre><code>$ tree operators/aqua\n\noperators/aqua\n\u251c\u2500\u2500 0.0.1\n...\n\u251c\u2500\u2500 catalog-templates\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 v4.12.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 v4.13.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 v4.14.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 v4.15.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 v4.16.yaml\n\u251c\u2500\u2500 ci.yaml\n</code></pre> <p>... and File-based catalog in <code>catalogs</code> directory</p> <pre><code>$ tree (repository root)/catalogs\ncatalogs\n\u251c\u2500\u2500 v4.12\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 aqua\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 catalog.yaml\n\u251c\u2500\u2500 v4.13\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 aqua\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 catalog.yaml\n\u251c\u2500\u2500 v4.14\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 aqua\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 catalog.yaml\n\u251c\u2500\u2500 v4.15\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 aqua\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 catalog.yaml\n\u2514\u2500\u2500 v4.16\n    \u2514\u2500\u2500 aqua\n        \u2514\u2500\u2500 catalog.yaml\n\n</code></pre>"},{"location":"users/fbc_onboarding/#submit-fbc-changes","title":"Submit FBC changes","text":"<p>Artifacts generated in the previous step need to be added to a git and submitted via pull request. The operator pipeline validates the content of the catalogs and releases changes into ocp catalogs.</p> <pre><code>$ git add operators/aqua/{catalog-templates,ci.yaml,Makefile}\n\n$ git add catalogs/{v4.12,v4.13,v4.14,v4.15,v4.16}/aqua\n\n$ git commit --signoff -m \"Add FBC resources for aqua operator\"\n</code></pre>"},{"location":"users/fbc_onboarding/#generating-catalogs-from-templates","title":"Generating catalogs from templates","text":"<p>Catalog templates are used to simplify a view of a catalog and allow easier manipulation of catalogs. The automated conversion pre-generates a basic template that can be turned into full FBC using the following command:</p> <pre><code>make catalogs\n</code></pre> <p>Of course, you can choose any type of template that you prefer by modifying the Makefile target. More information about catalog templates can be found here</p>"},{"location":"users/fbc_workflow/","title":"FBC workflow","text":"<p>If you already have an existing non-FBC operator please continue with the onboarding documentation to convert it to FBC. Once you have converted your operator, or you want to introduce a brand new operator, you can start with the FBC workflow.</p>"},{"location":"users/fbc_workflow/#fbc-operator-config","title":"FBC operator config","text":"<p>To indicate the operator is using fbc workflow an operator owner needs to indicate this fact in the <code>ci.yaml</code> file.</p> <p>Example of the <code>ci.yaml</code> with FBC config:</p> <pre><code>---\nfbc:\n  enabled: true\n</code></pre>"},{"location":"users/fbc_workflow/#fbc-templates","title":"FBC templates","text":"<p>File-based catalog templates serve as a simplified view of a catalog that can be updated by the user. The OPM currently supports 2 types of templates and it is up to the user which template the operator will be using.</p> <ul> <li>Basic template - <code>olm.template.basic</code></li> <li>SemVer template - <code>olm.semver</code></li> </ul> <p>More information about each template can be found at opm doc.</p> <p>The recommended template from the maintainability point of view is <code>SemVer</code>.</p>"},{"location":"users/fbc_workflow/#fbc-template-mapping","title":"FBC template mapping","text":"<p>To be able to generate a catalog from templates a user needs to provide a mapping between template and catalog. The mapping is stored in the <code>ci.yaml</code> file. Based on your preference you can map a template to a catalog version with 1:N mapping or 1:1 mapping.</p> <p>Here is an example of the <code>ci.yaml</code> file with single template rendering multiple catalogs (<code>1:N</code>):</p> <pre><code>---\nfbc:\n  enabled: true\n  catalog_mapping:\n    - template_name: my-custom-semver-template.yaml # The name of the file inside ./catalog-templates directory\n      catalogs_names: # a list of catalogs within the /catalogs directory\n        - \"v4.15\"\n        - \"v4.16\"\n        - \"v4.17\"\n      type: olm.semver\n    - template_name: my-custom-basic-template.yaml # The name of the file inside catalog-templates directory\n      catalogs_names:\n        - \"v4.12\"\n        - \"v4.13\"\n      type: olm.template.basic\n</code></pre> <p>And here is an example of the <code>ci.yaml</code> file with a single template rendering a single catalog (<code>1:1</code>):</p> <pre><code>---\nfbc:\n  enabled: true\n  catalog_mapping:\n  - template_name: v4.14.yaml\n    catalog_names: [\"v4.14\"]\n    type: olm.template.basic\n  - template_name: v4.15.yaml\n    catalog_names: [\"v4.15\"]\n    type: olm.template.basic\n  - template_name: v4.16.yaml\n    catalog_names: [\"v4.16\"]\n    type: olm.template.basic\n  - template_name: v4.17.yaml\n    catalog_names: [\"v4.17\"]\n    type: olm.template.basic\n</code></pre>"},{"location":"users/fbc_workflow/#generate-catalogs-using-templates","title":"Generate catalogs using templates","text":"<p>To generate a final catalog for an operator a user needs to execute different <code>opm</code> commands based on the template type. We as operator pipeline maintainers want to simplify this process and we prepared a <code>Makefile</code> with all pre-configured targets.</p> <p>To get the <code>Makefile</code> follow these steps (In case you converted the existing operator and followed the onboarding guide the <code>Makefile</code> should be already in your operator directory and you can skip the step.)</p> <pre><code>cd &lt;operator-repo&gt;/operator/&lt;operator-name&gt;\nwget https://raw.githubusercontent.com/redhat-openshift-ecosystem/operator-pipelines/main/fbc/Makefile\n</code></pre> <p>The right place for the Makefile is in the operator's root directory</p> <pre><code>.\n\u251c\u2500\u2500 0.0.1\n|   \u251c\u2500\u2500 release-config.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 manifests\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 metadata\n\u251c\u2500\u2500 catalog-templates\n\u251c\u2500\u2500 ci.yaml\n\u2514\u2500\u2500 Makefile\n\n</code></pre> <p>[!IMPORTANT] In case an operator isn't shipped to all OCP catalog versions manually update <code>OCP_VERSIONS</code> variable in the <code>Makefile</code> and include only versions supported by an operator.</p> <p>The command uses the <code>opm</code> and converts templates into catalogs. The generated catalogs can be submitted as a PR in Github and once the PR is processed changes will be released to the OCP index.</p> <pre><code>$ tree (repository-root)/catalogs\ncatalogs\n\u251c\u2500\u2500 v4.12\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 aqua\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 catalog.yaml\n\u251c\u2500\u2500 v4.13\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 aqua\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 catalog.yaml\n\u251c\u2500\u2500 v4.14\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 aqua\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 catalog.yaml\n\u251c\u2500\u2500 v4.15\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 aqua\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 catalog.yaml\n\u2514\u2500\u2500 v4.16\n    \u2514\u2500\u2500 aqua\n        \u2514\u2500\u2500 catalog.yaml\n\n</code></pre>"},{"location":"users/fbc_workflow/#adding-new-bundle-to-catalog","title":"Adding new bundle to Catalog","text":"<p>A new bundle can be added automatically to your templates and catalogs if you use the automated release feature. The process is described in the fbc auto-release documentation.</p> <p>It is highly recommended to use the automated release feature as it simplifies the process from the user perspective.</p> <p>However if you want to manually add a new bundle to the catalog follow the steps below.</p> <p>To add a bundle to the catalog you need to first submit the new version of the operator using traditional PR workflow. The operator pipeline builds, tests, and releases the bundle into the registry. At this point, the operator is not available in the catalog yet. To add the bundle to the catalog you need to update catalog templates and add a bundle pullspec given by pull request comment and open a new pull request with catalog changes.</p> <p></p> <p>[!NOTE] Currently a workflow requires a 2-step process to release a new bundle into the catalog. In the first step, the operator bundle is released and in the second step, the catalog is updated with the new bundle. We are working on a solution to automate this process and make it a single step. However, this will require a usage of <code>SemVer</code> catalog template. In case you would like to use this feature once available please consider using <code>SemVer</code> template.</p>"},{"location":"users/fbc_workflow/#semver","title":"SemVer","text":"<p>For example if I want to add <code>v1.1.0</code> bundle into <code>Fast</code> channel of a specific catalog I'll add it as mentioned in the example below:</p> <pre><code>---\nSchema: olm.semver\nGenerateMajorChannels: true\nGenerateMinorChannels: true\nCandidate:\n  Bundles:\n  - Image: quay.io/foo/olm:testoperator.v0.1.0\n  - Image: quay.io/foo/olm:testoperator.v0.1.1\n  - Image: quay.io/foo/olm:testoperator.v0.1.2\n  - Image: quay.io/foo/olm:testoperator.v0.1.3\n  - Image: quay.io/foo/olm:testoperator.v0.2.0\n  - Image: quay.io/foo/olm:testoperator.v0.2.1\n  - Image: quay.io/foo/olm:testoperator.v0.2.2\n  - Image: quay.io/foo/olm:testoperator.v0.3.0\n  - Image: quay.io/foo/olm:testoperator.v1.0.0\n  - Image: quay.io/foo/olm:testoperator.v1.0.1\n  - Image: quay.io/foo/olm:testoperator.v1.1.0\nFast:\n  Bundles:\n  - Image: quay.io/foo/olm:testoperator.v0.2.1\n  - Image: quay.io/foo/olm:testoperator.v0.2.2\n  - Image: quay.io/foo/olm:testoperator.v0.3.0\n  - Image: quay.io/foo/olm:testoperator.v1.0.0\n  - Image: quay.io/foo/olm:testoperator.v1.1.0 # &lt;-- Add new bundle into fast channel\nStable:\n  Bundles:\n  - Image: quay.io/foo/olm:testoperator.v1.0.0\n</code></pre> <p>Also see opm doc for automate-able step.</p>"},{"location":"users/fbc_workflow/#basic","title":"Basic","text":"<p>For example, if I want to add <code>v0.2.0</code> bundle into <code>stable</code> channel of specific catalog I'll add it as mentioned in the example below.</p> <ol> <li>Add a new <code>olm.bundle</code> entry with bundle pullspec</li> <li>Add bundle into the <code>stable</code> channel</li> </ol> <pre><code>---\nschema: olm.template.basic\nentries:\n  - schema: olm.package\n    name: example-operator\n    defaultChannel: stable\n\n  - schema: olm.channel\n    package: example-operator\n    name: stable\n    entries:\n      - name: example-operator.v0.1.0\n      - name: example-operator.v0.2.0 # &lt;-- Add bundle into channel\n        replaces: example-operator.v0.1.0\n\n  - schema: olm.bundle\n    image: docker.io/example/example-operator-bundle:0.1.0\n\n  - schema: olm.bundle # &lt;-- Add new bundle entry\n    image: docker.io/example-operator-bundle:0.2.0\n</code></pre> <p>Also see opm doc for automate-able step.</p>"},{"location":"users/fbc_workflow/#updating-existing-catalogs","title":"Updating existing catalogs","text":"<p>A great benefit of FBC is that users can update operator update graphs independently of operator releases. This allows any post-release modification of the catalogs. If you want to change the order of updates, remove an invalid bundle, or do any other modification you are free to do that.</p> <p>After updating catalog templates don't forget to run <code>make catalogs</code> to generate a catalog from templates and submit the resulting catalog using PR workflow.</p>"},{"location":"users/isv_pipelines/","title":"ISV operators","text":""},{"location":"users/isv_pipelines/#ciyaml-config","title":"ci.yaml config","text":"<p>Each operator submitted as a certified or marketplace operator needs to contains a <code>ci.yaml</code> config file that is used during the certification.</p> <p>The correct location of this file is at <code>operators/operator-XYZ/ci.yaml</code> and needs to contains at least following values:</p> <pre><code>---\n# The ID of certification component as stated in Red Hat Connect\ncert_project_id: &lt;certification project id&gt;\n\n</code></pre> <p>Other optional value is <code>merge: false</code> that prevents from automatically merging a pull request with an operator if all tests passes. The default behavior is to merge a PR automatically.</p>"},{"location":"users/operator-ci-yaml/","title":"Operator Publishing / Review settings","text":"<p>Each operator might have <code>ci.yaml</code> configuration file to be present in an operator directory (for example <code>operators/aqua/ci.yaml</code>). This configuration file is used by the pipeline automation to control a way how the operator will be published and reviewed.</p> <p>A content of the file depends on the operator source type. There are a different set of options for community operators and certified operators.</p> <p>Note:     One can create or modify <code>ci.yaml</code> file with a new operator version. This operation can be done in the same PR with other operator changes.</p>"},{"location":"users/operator-ci-yaml/#reviewers","title":"Reviewers","text":"<p>Note:     This option is only valid for community operators. The certified or marketplace reviewer are configure using Red Hat Connect.</p> <p>If you want to accelerate publishing your changes, consider adding yourself and others you trust to the <code>reviewers</code> list. If the author of PR will be in that list, changes she/he made will be taken as authorized changes. This will be the indicator for our pipeline that the PR is ready to merge automatically.</p> <p>Note:     If an author of PR is not in <code>reviewers</code> list or not in <code>ci.yaml</code> on <code>main</code> branch, PR will not be merged automatically.</p> <p>Note:     If an author of PR is not in <code>reviewers</code> list and <code>reviewers</code> are present in <code>ci.yaml</code> file. All <code>reviewers</code> will be mentioned in PR comment to check for upcoming changes.</p> <p>For this to work, it is required to setup reviewers in <code>ci.yaml</code> file. It can be done by adding <code>reviewers</code> tag with a list of GitHub usernames. For example</p>"},{"location":"users/operator-ci-yaml/#example","title":"Example","text":"<pre><code>$ cat &lt;path-to-operator&gt;/ci.yaml\n---\nreviewers:\n  - user1\n  - user2\n\n</code></pre>"},{"location":"users/operator-ci-yaml/#fbc-mode","title":"FBC mode","text":""},{"location":"users/operator-ci-yaml/#fbcenabled","title":"<code>fbc.enabled</code>","text":"<p>The <code>fbc.enabled</code> flag enables the File-Based catalog feature. It is highly recommended to use the FBC mode in order to have better control over the operator's catalog.</p>"},{"location":"users/operator-ci-yaml/#fbcversion_promotion_strategy","title":"<code>fbc.version_promotion_strategy</code>","text":"<p>The <code>fbc.version_promotion_strategy</code> option defines the strategy for promoting the operator into a next OCP version. When a new OCP version becomes available an automated process will promote the operator from a version N to a version N+1. The <code>fbc.version_promotion_strategy</code> option can have the following values:</p> <ul> <li><code>review-needed</code> - the operator will be promoted to the next OCP version automatically, but the PR will be created and the reviewers will be asked to review the changes (default)</li> <li><code>always</code> - the operator will be promoted to the next OCP version automatically</li> <li><code>never</code> - the operator will not be promoted to the next OCP version automatically</li> </ul>"},{"location":"users/operator-ci-yaml/#fbccatalog_mapping","title":"<code>fbc.catalog_mapping</code>","text":"<p>The mapping serves as a link between catalog templates within the <code>./catalog-templates</code> directory and catalogs within the <code>./catalogs</code> directory.</p> <p>For more details and structure visit the FBC workflow page.</p>"},{"location":"users/operator-ci-yaml/#example_1","title":"Example","text":"<pre><code>---\nfbc:\n  enabled: true\n  version_promotion_strategy: never\n  catalog_mapping:\n    - template_name: my-custom-semver-template.yaml # The name of the file inside ./catalog-templates directory\n        catalogs_names: # a list of catalogs within the /catalogs directory\n          - \"v4.15\"\n          - \"v4.16\"\n          - \"v4.17\"\n        type: olm.semver\n    - template_name: my-custom-basic-template.yaml # The name of the file inside catalog-templates directory\n        catalogs_names:\n          - \"v4.12\"\n          - \"v4.13\"\n        type: olm.template.basic\n</code></pre>"},{"location":"users/operator-ci-yaml/#operator-versioning","title":"Operator versioning","text":"<p>NOTE: This option is only available for the non-FBC operators where user doesn't have a direct control over the catalog.</p> <p>Operators have multiple versions. When a new version is released, OLM can update an operator automatically. There are 2 update strategies possible, which are defined in <code>ci.yaml</code> at the operator top level.</p>"},{"location":"users/operator-ci-yaml/#replaces-mode","title":"replaces-mode","text":"<p>Every next version defines which version will be replaced using <code>replaces</code> key in the CSV file. It means, that there is a possibility to omit some versions from the update graph. The best practice is to put them in a separate channel then.</p>"},{"location":"users/operator-ci-yaml/#semver-mode","title":"semver-mode","text":"<p>Every version will be replaced by the next higher version according to semantic versioning.</p>"},{"location":"users/operator-ci-yaml/#restrictions","title":"Restrictions","text":"<p>A contributor can decide if <code>semver-mode</code> or <code>replaces-mode</code> mode will be used for a specific operator. By default, <code>replaces-mode</code> is activated, when <code>ci.yaml</code> file is present and contains <code>updateGraph: replaces-mode</code>. When a contributor decides to switch and use <code>semver-mode</code>, it will be specified in <code>ci.yaml</code> file or the key <code>updateGraph</code> will be missing.</p>"},{"location":"users/operator-ci-yaml/#example_2","title":"Example","text":"<pre><code>$ cat &lt;path-to-operator&gt;/ci.yaml\n---\n# Use `replaces-mode` or `semver-mode`.\nupdateGraph: replaces-mode\n</code></pre>"},{"location":"users/operator-ci-yaml/#certification-project","title":"Certification project","text":""},{"location":"users/operator-ci-yaml/#cert_project_id","title":"<code>cert_project_id</code>","text":"<p>The <code>cert_project_id</code> option is required for certified and marketplace operators. It is used to link the operator to the certification project in Red Hat Connect.</p>"},{"location":"users/operator-ci-yaml/#kubernetes-max-version-in-csv","title":"Kubernetes max version in CSV","text":"<p>Starting from kubernetes 1.22 some old APIs were deprecated (Deprecated API Migration Guide from v1.22. Users can set <code>operatorhub.io/ui-metadata-max-k8s-version: \"&lt;version&gt;\"</code> in its CSV file to inform its maximum supported Kubernetes version. The following example will inform that operator can handle <code>1.21</code> as maximum Kubernetes version</p> <pre><code>$ cat &lt;path-to-operators&gt;/&lt;name&gt;/&lt;version&gt;/.../my.clusterserviceversion.yaml\nmetadata:\n  annotations:\n    operatorhub.io/ui-metadata-max-k8s-version: \"1.21\"\n</code></pre>"},{"location":"users/packaging-required-criteria-ocp/","title":"OKD/OpenShift Catalogs criteria and options","text":""},{"location":"users/packaging-required-criteria-ocp/#okdopenshift-catalogs-criteria-and-options","title":"OKD/OpenShift Catalogs criteria and options","text":""},{"location":"users/packaging-required-criteria-ocp/#overview","title":"Overview","text":"<p>To distribute on OpenShift Catalogs, you will need to comply with the same standard criteria defined for <code>OperatorHub.io</code> (see Common recommendations and suggestions). Then, additionally, you have some requirements and options which follow.</p> <p>IMPORTANT Kubernetes has been deprecating API(s) which will be removed and no longer available in <code>1.22</code> and in the Openshift version <code>4.9</code>. Note that your project will be unable to use them on <code>OCP 4.9/K8s 1.22</code> and then, it is strongly recommended to check Deprecated API Migration Guide from v1.22 and ensure that your projects have them migrated and are not using any deprecated API.</p> <p>Note that your operator using them will not work in  <code>1.22</code> and in the Openshift version <code>4.9</code>. OpenShift 4.8 introduces two new alerts that fire when an API that will be removed in the next release is in use. Check the event alerts of your Operators running on 4.8 and ensure that you will not find any warning about these API(s) still being used by it.</p> <p>Also, to prevent workflow issues, its users will need to have installed in their OCP cluster a version of your operator compatible with 4.9 before they try to upgrade their cluster from any previous version to 4.9 or higher. In this way, it is recommended to ensure that your operators are no longer using these API(s) versions. However, If you still need to publish the operator bundles with any of these API(s) for use on earlier k8s/OCP versions, ensure that the operator bundle is configured accordingly.</p> <p>Taking the actions below will help prevent users from installing versions of your operator on an incompatible version of OCP, and also prevent them from upgrading to a newer version of OCP that would be incompatible with the version of your operator that is currently installed on their cluster.</p>"},{"location":"users/packaging-required-criteria-ocp/#configure-the-max-openshift-version-compatible","title":"Configure the max OpenShift Version compatible","text":"<p>Use the <code>olm.openShiftMaxVersion</code> annotation in the CSV to prevent the user from upgrading their OCP cluster before upgrading the installed operator version to any distribution which is compatible with:</p> <pre><code>apiVersion: operators.coreos.com/v1alpha1\nkind: ClusterServiceVersion\nmetadata:\n  annotations:\n    # Prevent cluster upgrades to OpenShift Version 4.9 when this\n    # bundle is installed on the cluster\n    \"olm.properties\": '[{\"type\": \"olm.maxOpenShiftVersion\", \"value\": \"4.8\"}]'\n</code></pre> <p>The CSV annotation will eventually prevent the user from upgrading their OCP cluster before they have installed a version of your operator which is compatible with <code>4.9</code>. However, note that it is important to make these changes now as users running workloads with deprecated API(s) that are looking to upgrade to OCP 4.9 will need to be running operators that have this annotation set in order to prevent the cluster upgrade and potentially adversely impacting their crucial workloads.</p> <p>This option is useful when you know that the current version of your project will not work well on some specific Openshift version.</p>"},{"location":"users/packaging-required-criteria-ocp/#configure-the-openshift-distribution","title":"Configure the Openshift distribution","text":"<p>Use the annotation <code>com.redhat.openshift.versions</code> in <code>bundle/metadata/annotations.yaml</code> to ensure that the index image will be generated with its OCP Label, to prevent the bundle from being distributed on to 4.9:</p> <pre><code>com.redhat.openshift.versions: \"v4.6-v4.8\"\n</code></pre> <p>This option is also useful when you know that the current version of your project will not work well on some specific OpenShift version. By using it you defined the Openshift versions where the Operator should be distributed and the Operator will not appear in a catalog of an Openshift version that is outside of the range. You must use it if you are distributing a solution that contains deprecated API(s) and will no longer be available in later versions. For more information see Managing OpenShift Versions.</p>"},{"location":"users/packaging-required-criteria-ocp/#validate-the-bundle-with-the-common-criteria-to-distribute-via-olm-with-sdk","title":"Validate the bundle with the common criteria to distribute via OLM with SDK","text":"<p>Also, you can check the bundle via <code>operator-sdk bundle validate</code> against the suite  Validator Community Operators and the K8s Version that you are intended to publish:</p> <pre><code>operator-sdk bundle validate ./bundle --select-optional suite=operatorframework --optional-values=k8s-version=1.22\n</code></pre> <p>NOTE: The validators only check the manifests which are shipped in the bundle. They are unable to ensure that the project's code does not use the Deprecated/Removed API(s) in 1.22 and/or that it does not have as dependency another operator that uses them.</p>"},{"location":"users/packaging-required-criteria-ocp/#validate-the-bundle-with-the-specific-criteria-to-distribute-in-openshift-catalogs","title":"Validate the bundle with the specific criteria to distribute in Openshift catalogs","text":"<p>Pre-requirement Download the binary. You might want to keep it in your <code>$GOPTH/bin</code></p> <p>Then, we can use the experimental OpenShift OLM Catalog Validator to check your Operator bundle. In this case, we need to inform the bundle and the annotations.yaml file paths:</p> <pre><code>$ ocp-olm-catalog-validator my-bundle-path/bundle  --optional-values=\"file=bundle-path/bundle/metadata/annotations.yaml\"\n</code></pre> <p>Following is an example of an Operator bundle that uses the removed APIs in 1.22 and is not configured accordingly:</p> <pre><code>$ ocp-olm-catalog-validator bundle/ --optional-values=\"file=bundle/metadata/annotations.yaml\"\nWARN[0000] Warning: Value memcached-operator.v0.0.1: this bundle is using APIs which were deprecated and removed in v1.22. More info: https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-22. Migrate the API(s) for CRD: ([\"memcacheds.cache.example.com\"])\nERRO[0000] Error: Value : (memcached-operator.v0.0.1) olm.maxOpenShiftVersion csv.Annotations not specified with an OCP version lower than 4.9. This annotation is required to prevent the user from upgrading their OCP cluster before they have installed a version of their operator which is compatible with 4.9. For further information see https://docs.openshift.com/container-platform/4.8/operators/operator_sdk/osdk-working-bundle-images.html#osdk-control-compat_osdk-working-bundle-images\nERRO[0000] Error: Value : (memcached-operator.v0.0.1) this bundle is using APIs which were deprecated and removed in v1.22. More info: https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-22. Migrate the APIs for this bundle is using APIs which were deprecated and removed in v1.22. More info: https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-22. Migrate the API(s) for CRD: ([\"memcacheds.cache.example.com\"]) or provide compatible version(s) via the labels. (e.g. LABEL com.redhat.openshift.versions='4.6-4.8')\n</code></pre>"},{"location":"users/pipelines_overview/","title":"Overview","text":"<p>Operator pipelines is a Tekton based solution that serves as a CI/CD platform for Operators targeting Red Hat Openshift platform. The CI/CD process makes sure all operators available in the OCP met certain standards.</p> <p>The series of pipelines validates the operator, tests it and make it available to all OCP users. In combination with Github repository users or partners are able to submit a new operator in Pull request workflow and release it.</p>"},{"location":"users/pipelines_overview/#operator-repositories","title":"Operator repositories","text":"<p>The Openshift platform include by default several catalogs from which users can install an operator. This CI/CD solution aims for Red Hat Partners or Community members. Thus there are 3 separate repositories where operator owner can submit operators. Each repository serves a different purpose and has its own specific rules but is shares the common CI/CD solution.</p> <ul> <li>Certified operators</li> <li>Marketplace operators</li> <li>Community OCP operators</li> </ul>"},{"location":"users/pipelines_overview/#sequence-diagram","title":"Sequence diagram","text":"<pre><code>sequenceDiagram\n    Operator Owner-&gt;&gt;Github: Submits a PR with operator\n    Github-&gt;&gt;Pipeline: Triggers a hosted pipeline\n    Pipeline-&gt;&gt;Pipeline: Execute tests and validation\n    Pipeline-&gt;&gt;Github: Merge PR\n    Github-&gt;&gt;Pipeline: Triggers a release pipeline\n    Pipeline-&gt;&gt;Pipeline: Release operator to OCP catalog\n    Pipeline-&gt;&gt;Github: Notify user in PR\n\n</code></pre>"},{"location":"users/static_checks/","title":"Static check","text":"<p>The operator pipelines want to make sure the operator that will be released to OpenShift operator catalog follows a best practices and meet certain standards that we expect from an operator.</p> <p>In order to meet these standards a series of static checks have been created for each stream of operators. The static checks are executed for each operator submission and reports warning or failures with an description of what is wrong and suggestion on how to fix it.</p> <p>Here is the example how the result will look like in the PR:</p> <p></p>"},{"location":"users/static_checks/#isv-tests","title":"ISV tests","text":""},{"location":"users/static_checks/#check_pruned_graph-warning","title":"check_pruned_graph (Warning)","text":"<p>This test make sure the operator update graph is not accidentally pruned by introducing operator configuration that prunes graph as a unwanted side effect.</p> <p>The unintentional graph pruning happens when olm.skipRange annotation is set but replaces field is not set in the CSV. The definition may lead to unintentional pruning of the update graph.</p> <p>If this is intentional, you can skip the check by adding <code>/test skip check_pruned_graph</code> comment to a pull request.</p>"},{"location":"users/static_checks/#check_marketplace_annotation","title":"check_marketplace_annotation","text":"<p>The marketplace operators requires additional metadata in order to be properly displayed in the Marketplace ecosystem.</p> <p>There are 2 required fields in the operator <code>clusterserviceversion</code> that need to be filled and need to have specific value:</p> <ul> <li><code>metadata.annotations.marketplace.openshift.io/remote-workflow</code><ul> <li>Value template: <code>https://marketplace.redhat.com/en-us/operators/{annotation_package}/pricing?utm_source=openshift_console</code></li> </ul> </li> <li><code>metadata.annotations.marketplace.openshift.io/support-workflow</code><ul> <li>Value template: <code>https://marketplace.redhat.com/en-us/operators/{annotation_package}/support?utm_source=openshift_console</code></li> </ul> </li> </ul> <p>Where <code>{annotation_package}</code> matches <code>operators.operatorframework.io.bundle.package.v1</code> from <code>metadata/annotation.yaml</code> file.</p> <p>The test is only executed for operators submitted inside the Red Hat marketplace repo.</p>"},{"location":"users/static_checks/#community-tests","title":"Community tests","text":""},{"location":"users/static_checks/#check_osdk_bundle_validate_operatorhub","title":"check_osdk_bundle_validate_operatorhub","text":"<p>The test is based on <code>operator-sdk bundle validate</code> command with <code>name=operatorhub</code> test suite (link).</p>"},{"location":"users/static_checks/#check_osdk_bundle_validate_operator_framework","title":"check_osdk_bundle_validate_operator_framework","text":"<p>The test is based on <code>operator-sdk bundle validate</code> command with <code>suite=operatorframework</code> test suite (link).</p>"},{"location":"users/static_checks/#check_required_fields","title":"check_required_fields","text":"Field name Validation Description <code>spec.displayName</code> <code>.{3,50}</code> A string with 3 - 50 characters <code>spec.description</code> <code>.{20,}</code> A bundle description with at least 20 characters <code>spec.icon</code> <code>media</code> A valid base64 content with a supported media type (<code>{\"base64data\": &lt;b64 content&gt;, \"mediatype\": enum[\"image/png\", \"image/jpeg\", \"image/gif\", \"image/svg+xml\"]}</code>) <code>spec.version</code> <code>SemVer</code> Valid semantic version <code>spec.maintainers</code> At least 1 maintainer contacts. Example: <code>{\"name\": \"User 123\", \"email\": \"user@redhat.com\"}</code> <code>spec.provider.name</code> <code>.{3,}</code> A string with at least 3 characters <code>spec.links</code> At least 1 link. Example: <code>{\"name\": \"Documentation\", \"url\": \"https://redhat.com\"}</code>"},{"location":"users/static_checks/#check_dangling_bundles","title":"check_dangling_bundles","text":"<p>The test prevents from releasing an operator and keeping any previous bundle dangling. A dangling bundle is a bundle that is not referenced by any other bundle and is not a HEAD of a channel.</p> <p>In the example bellow the <code>v1.3</code> bundle is dangling.</p> <pre><code>graph LR\n    A(v1.0) --&gt;B(v1.1)\n    B --&gt; C(v1.2)\n    B --&gt; E(v1.3)\n    C --&gt; D(v1.4 - HEAD)\n</code></pre>"},{"location":"users/static_checks/#check_api_version_constraints","title":"check_api_version_constraints","text":"<p>The test verifies a consistency between value <code>com.redhat.openshift.versions</code> from annotation with <code>spec.minKubeVersion</code>. In case a an operator targets specific version of OpenShift and at the same time sets minimal kube version that is higher than the one supported by the OCP. The test raises an error.</p> <p>Example:</p> <p>Following combination is not valid since the OCP 4.9 is based on 1.22 Kubernetes.</p> <pre><code>spec.minKubeVersion: 1.23\n\ncom.redhat.openshift.versions: 4.9-4.15\n</code></pre>"},{"location":"users/static_checks/#check_upgrade_graph_loop","title":"check_upgrade_graph_loop","text":"<p>The purpose of this test is to check whether there are any loops in the upgrade graph.</p> <p>As stated on the graph below the edge between <code>v1.2</code> and <code>v1.0</code> introduces a loop in the graph.</p> <pre><code>graph LR\n    A(v1.0) --&gt;B(v1.1)\n    B --&gt; C(v1.2)\n    C --&gt; D(v1.3)\n    C --&gt; A\n</code></pre>"},{"location":"users/static_checks/#check_replaces_availability","title":"check_replaces_availability","text":"<p>The test aims to verify if a bundle referenced by the <code>replaces</code> value is available in all catalog version where the given bundle is going to be released to. The list of catalog version is determined by the <code>com.redhat.openshift.versions</code> annotation if present. If the annotation is not present the bundle targets all supported ocp version.</p> <p>To fix the issue either change a range of versions where a bundle is going to be released by updating the annotation or change the <code>replaces</code> value.</p>"},{"location":"users/static_checks/#check_operator_name_unique","title":"check_operator_name_unique","text":"<p>The test makes sure the operator is consistent when using operator names as defined in the <code>clusterserviceversion</code>. It is not allowed to have multiple bundle names for a single operator. The source of the value is at <code>csv.metadata.name</code>.</p>"},{"location":"users/static_checks/#check_ci_upgrade_graph","title":"check_ci_upgrade_graph","text":"<p>The test verifies a content of the <code>ci.yaml</code> file and make sure only allowed values are used for <code>updateGraph</code> key. The currently supported values are: <code>[\"replaces-mode\", \"semver-mode\"]</code>.</p>"},{"location":"users/static_checks/#common-tests","title":"Common tests","text":""},{"location":"users/static_checks/#check_operator_name-warning","title":"check_operator_name (Warning)","text":"<p>The test verifies a consistency between operator name annotation and operator name in the CSV definition. The source of these values are:</p> <ul> <li><code>operators.operatorframework.io.bundle.package.v1</code> (<code>metadata/annotation.yaml</code>)</li> <li><code>csv.metadata.name</code> - the name without a version (<code>manifests/.*.clusterserviceversion.yaml</code>)</li> </ul>"},{"location":"users/static_checks/#check_bundle_images_in_fbc","title":"check_bundle_images_in_fbc","text":"<p>This check will ensure that all bundle images in the file based catalog for given operator catalog(s) use allowed image registry. Allowed registries are configured in <code>(repo_root)/config.yaml</code> under the key <code>allowed_bundle_registries</code>.</p>"},{"location":"users/static_checks/#check_schema_bundle_release_config","title":"check_schema_bundle_release_config","text":"<p>The test validates the <code>release-config.yaml</code> file against the schema. The file description including the schema definition can be found here.</p>"},{"location":"users/static_checks/#check_schema_operator_ci_config","title":"check_schema_operator_ci_config","text":"<p>The test validates the <code>ci.yaml</code> file against the schema. The schema definition can be foundhere.</p>"},{"location":"users/static_checks/#check_catalog_usage_ci_config","title":"check_catalog_usage_ci_config","text":"<p>The test makes sure the <code>fbc.catalog_mapping</code> in <code>ci.yaml</code> file is not mapping a single catalog to multiple catalog templates. The test will fail if the same catalog is used in multiple templates.</p> <p>Example of the <code>ci.yaml</code> file where <code>v4.14</code> catalog is used in two different templates.</p> <pre><code>---\nfbc:\n  enabled: true\n\n  catalog_mapping:\n    - template_name: basic.yaml\n      catalog_names: [\"v4.14\", \"v4.15\", \"v4.16\"]\n      type: olm.template.basic\n    - template_name: semver.yaml\n      catalog_names: [\"v4.13\", \"v4.14\"] # The 4.14 is already used in the basic.yaml template\n      type: olm.semver\n</code></pre>"},{"location":"users/static_checks/#check_olm_bundle_object_in_fbc","title":"check_olm_bundle_object_in_fbc","text":"<p>The test verifies a presence of <code>olm.bundle.object</code> in the catalog's bundle properties. For catalogs <code>&gt;=v4.17</code> the <code>olm.bundle.object</code> is not allowed as it has negative impact on performance of the OpenShift olm.</p> <p>The test will fail if the <code>olm.bundle.object</code> is present in the catalog within the <code>/catalogs/{ocp_version}/{operator_name}/catalog.yaml</code> file.</p> <p>To prevent the test from failing download the latest Makefile and re-render the catalog again with <code>make catalogs</code> command. The Makefile uses extra arguments <code>--migrate-level bundle-object-to-csv-metadata</code> for opm when rendering catalogs for <code>&gt;=4.17</code> version.</p>"},{"location":"users/static_checks/#running-tests-locally","title":"Running tests locally","text":"<pre><code># Install the package with static checks\n$ pip install git+https://github.com/redhat-openshift-ecosystem/operator-pipelines.git\n\n# Execute a test suite\n# In this example tests are executed for aqua operator\n# with 2022.4.15 version and two operator catalogs (v4.17/aqua and v4.18/aqua)\n$ python static-tests \\\n    --repo-path ~/community-operators-prod \\\n    --suites operatorcert.static_tests.community \\\n    --output-file /tmp/operator-test.json \\\n    --verbose \\\n    aqua 2022.4.15 v4.17/aqua,v4.18/aqua\n\n</code></pre> <pre><code>$ cat /tmp/operator-test.json | jq\n\n{\n  \"passed\": false,\n  \"outputs\": [\n    {\n      \"type\": \"error\",\n      \"message\": \"Channel 2022.4.0 has dangling bundles: {Bundle(aqua/2022.4.14)}\",\n      \"test_suite\": \"operatorcert.static_tests.community\",\n      \"check\": \"check_dangling_bundles\"\n    },\n    {\n      \"type\": \"warning\",\n      \"message\": \"Warning: Value aquasecurity.github.io/v1alpha1, Kind=ClusterConfigAuditReport: provided API should have an example annotation\",\n      \"test_suite\": \"operatorcert.static_tests.community\",\n      \"check\": \"check_osdk_bundle_validate_operator_framework\"\n    },\n    {\n      \"type\": \"warning\",\n      \"message\": \"Warning: Value aquasecurity.github.io/v1alpha1, Kind=AquaStarboard: provided API should have an example annotation\",\n      \"test_suite\": \"operatorcert.static_tests.community\",\n      \"check\": \"check_osdk_bundle_validate_operator_framework\"\n    },\n    {\n      \"type\": \"warning\",\n      \"message\": \"Warning: Value aquasecurity.github.io/v1alpha1, Kind=ConfigAuditReport: provided API should have an example annotation\",\n      \"test_suite\": \"operatorcert.static_tests.community\",\n      \"check\": \"check_osdk_bundle_validate_operator_framework\"\n    },\n    {\n      \"type\": \"warning\",\n      \"message\": \"Warning: Value : (aqua-operator.v2022.4.15) csv.Spec.minKubeVersion is not informed. It is recommended you provide this information. Otherwise, it would mean that your operator project can be distributed and installed in any cluster version available, which is not necessarily the case for all projects.\",\n      \"test_suite\": \"operatorcert.static_tests.community\",\n      \"check\": \"check_osdk_bundle_validate_operator_framework\"\n    },\n    {\n      \"type\": \"warning\",\n      \"message\": \"Warning: Value : (aqua-operator.v2022.4.15) csv.Spec.minKubeVersion is not informed. It is recommended you provide this information. Otherwise, it would mean that your operator project can be distributed and installed in any cluster version available, which is not necessarily the case for all projects.\",\n      \"test_suite\": \"operatorcert.static_tests.community\",\n      \"check\": \"check_osdk_bundle_validate_operator_framework\"\n    },\n    {\n      \"type\": \"warning\",\n      \"message\": \"Warning: Value aqua-operator.v2022.4.15: this bundle is using APIs which were deprecated and removed in v1.25. More info: https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-25. Migrate the API(s) for podsecuritypolicies: ([\\\"ClusterServiceVersion.Spec.InstallStrategy.StrategySpec.ClusterPermissions[2].Rules[7]\\\"])\",\n      \"test_suite\": \"operatorcert.static_tests.community\",\n      \"check\": \"check_osdk_bundle_validate_operator_framework\"\n    },\n    {\n      \"type\": \"warning\",\n      \"message\": \"Warning: Value aqua-operator.v2022.4.15: this bundle is using APIs which were deprecated and removed in v1.25. More info: https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-25. Migrate the API(s) for podsecuritypolicies: ([\\\"ClusterServiceVersion.Spec.InstallStrategy.StrategySpec.ClusterPermissions[2].Rules[7]\\\"])\",\n      \"test_suite\": \"operatorcert.static_tests.community\",\n      \"check\": \"check_osdk_bundle_validate_operator_framework\"\n    },\n    {\n      \"type\": \"warning\",\n      \"message\": \"Warning: Value aqua-operator.v2022.4.15: unable to find the resource requests for the container: (aqua-operator). It is recommended to ensure the resource request for CPU and Memory. Be aware that for some clusters configurations it is required to specify requests or limits for those values. Otherwise, the system or quota may reject Pod creation. More info: https://master.sdk.operatorframework.io/docs/best-practices/managing-resources/\",\n      \"test_suite\": \"operatorcert.static_tests.community\",\n      \"check\": \"check_osdk_bundle_validate_operator_framework\"\n    },\n    {\n      \"type\": \"warning\",\n      \"message\": \"Warning: Value aquasecurity.github.io/v1alpha1, Kind=ConfigAuditReport: provided API should have an example annotation\",\n      \"test_suite\": \"operatorcert.static_tests.community\",\n      \"check\": \"check_osdk_bundle_validate_operatorhub\"\n    },\n    {\n      \"type\": \"warning\",\n      \"message\": \"Warning: Value : (aqua-operator.v2022.4.15) csv.Spec.minKubeVersion is not informed. It is recommended you provide this information. Otherwise, it would mean that your operator project can be distributed and installed in any cluster version available, which is not necessarily the case for all projects.\",\n      \"test_suite\": \"operatorcert.static_tests.community\",\n      \"check\": \"check_osdk_bundle_validate_operatorhub\"\n    },\n    {\n      \"type\": \"warning\",\n      \"message\": \"Warning: Value : (aqua-operator.v2022.4.15) csv.Spec.minKubeVersion is not informed. It is recommended you provide this information. Otherwise, it would mean that your operator project can be distributed and installed in any cluster version available, which is not necessarily the case for all projects.\",\n      \"test_suite\": \"operatorcert.static_tests.community\",\n      \"check\": \"check_osdk_bundle_validate_operatorhub\"\n    },\n    {\n      \"type\": \"warning\",\n      \"message\": \"Warning: Value aqua-operator.v2022.4.15: this bundle is using APIs which were deprecated and removed in v1.25. More info: https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-25. Migrate the API(s) for podsecuritypolicies: ([\\\"ClusterServiceVersion.Spec.InstallStrategy.StrategySpec.ClusterPermissions[2].Rules[7]\\\"])\",\n      \"test_suite\": \"operatorcert.static_tests.community\",\n      \"check\": \"check_osdk_bundle_validate_operatorhub\"\n    },\n    {\n      \"type\": \"warning\",\n      \"message\": \"Warning: Value : The \\\"operatorhub\\\" validator is deprecated; for equivalent validation use \\\"operatorhub/v2\\\", \\\"standardcapabilities\\\" and \\\"standardcategories\\\" validators\",\n      \"test_suite\": \"operatorcert.static_tests.community\",\n      \"check\": \"check_osdk_bundle_validate_operatorhub\"\n    },\n    {\n      \"type\": \"error\",\n      \"message\": \"Invalid bundle image(s) found in OperatorCatalog(v4.17/aqua):\n      quay.io/invalid-repository/aqua@sha256:123. Only these registries are allowed for bundle images: quay.io/allowed-repository/, registry.connect.redhat.com/.\",\n      \"test_suite\": \"operatorcert.static_tests.common\",\n      \"check\": \"check_bundle_images_in_fbc\"\n    }\n  ]\n}\n\n</code></pre>"}]}